{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data1: TF-IDF + Short Features.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vDmkqhWGANkO",
        "b_pYFLPhoi3r",
        "UnB1RiyS_wOi",
        "8-9LY20iLIok"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7abQ3i8tAAA"
      },
      "source": [
        "# Hybrid Recommender using Deep Neural Network\n",
        "\n",
        "*Side note:* <br>\n",
        "Tensorflow version is 1.15. <br>\n",
        "*Lazy* approach. Hard to maintain but efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-PuxGveQBcA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6mHTKCEPzCl",
        "outputId": "f828b633-db96-438c-bdbb-b0d7a4bc8f7d"
      },
      "source": [
        "!gdown --id '1Y8MIETuWdWOGWAwkg65gps2est1OHa6P'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Y8MIETuWdWOGWAwkg65gps2est1OHa6P\n",
            "To: /content/Data_1 (TF-IDF + Little_Listing_Features + SVD).csv\n",
            "195MB [00:01, 106MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEgSlAVQQ_k"
      },
      "source": [
        "df_cross = pd.read_csv('/content/Data_1 (TF-IDF + Little_Listing_Features + SVD).csv').iloc[:,1:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN0MWpL2QcNr"
      },
      "source": [
        "cols = df_cross.columns.to_list()\n",
        "\n",
        "cols = map(lambda x:x.replace(' ','_'),cols)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCE2horkXBuV"
      },
      "source": [
        "df_cross.columns = cols"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEx8MHAlqvT"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMHS1jztLMX"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3rP05AfPgSB"
      },
      "source": [
        "# Determine csv and label columns\n",
        "num_factor = 10\n",
        "text_feat = 100\n",
        "\n",
        "NON_FACTOR_COLUMNS = [\"preference\",'listing_id',\n",
        "       'host_is_superhost', 'latitude',\n",
        "       'longitude','price','number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "       'nearest_station_dist', 'host_response_time_cross_rate']\n",
        "FACTOR_COLUMNS1 = [\"user_factor_{}\".format(i) for i in range(num_factor)] + [\"item_factor_{}\".format(i) for i in range(num_factor)]\n",
        "FACTOR_COLUMNS2 = [\"text_factor_{}\".format(i) for i in range(text_feat)] \n",
        "CSV_COLUMNS = NON_FACTOR_COLUMNS + FACTOR_COLUMNS1 + FACTOR_COLUMNS2\n",
        "LABEL_COLUMN = \"preference\"\n",
        "\n",
        "# Set default values for each CSV column\n",
        "NON_FACTOR_DEFAULTS = [[0.0],['Unknown'],[0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
        "FACTOR_DEFAULTS1 = [[0.0] for i in range(num_factor)] + [[0.0] for i in range(num_factor)] # user and item\n",
        "FACTOR_DEFAULTS2 = [[0.0] for i in range(text_feat)]\n",
        "DEFAULTS = NON_FACTOR_DEFAULTS + FACTOR_DEFAULTS1 + FACTOR_DEFAULTS2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hO08m-250NT",
        "outputId": "f81f9a97-1420-452d-bc8c-8f76596a85de"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest = train_test_split(df_cross, test_size=0.2, random_state=1)\n",
        "Xtrain, Xval = train_test_split(Xtrain, test_size=0.2, random_state=1)\n",
        "print(f\"Shape of train data: {Xtrain.shape}\")\n",
        "print(f\"Shape of validation data: {Xval.shape}\")\n",
        "print(f\"Shape of test data: {Xtest.shape}\")\n",
        "\n",
        "Xtrain.to_csv('train.csv',header=False,index=False)\n",
        "Xval.to_csv('val.csv',header=False,index=False)\n",
        "Xtest.to_csv('test.csv',header=False,index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train data: (76640, 132)\n",
            "Shape of validation data: (19161, 132)\n",
            "Shape of test data: (23951, 132)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjuROnkB2t-Q"
      },
      "source": [
        "### Make auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19vsrcrdCwfJ"
      },
      "source": [
        "lat_min = df_cross.latitude.min()\n",
        "lat_max = df_cross.latitude.max()\n",
        "\n",
        "lon_min = df_cross.longitude.min()\n",
        "lon_max = df_cross.longitude.max()\n",
        "\n",
        "norl_mean = df_cross.number_of_reviews_ltm.mean()\n",
        "norl_std = df_cross.number_of_reviews_ltm.std()\n",
        "\n",
        "chlc_mean = df_cross.calculated_host_listings_count.mean()\n",
        "chlc_std = df_cross.calculated_host_listings_count.std()\n",
        "\n",
        "nsr_mean = df_cross.nearby_sites_rating.mean()\n",
        "nsr_std = df_cross.nearby_sites_rating.std()\n",
        "\n",
        "nsd_mean = df_cross.nearest_station_dist.mean()\n",
        "nsd_std = df_cross.nearest_station_dist.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-eeQWi_f-rS"
      },
      "source": [
        "# Create input function for train and eval\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "    def _input_fn(): \n",
        "        def decode_csv(value_column):\n",
        "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
        "            features = dict(zip(CSV_COLUMNS, columns))          \n",
        "            label = features.pop(LABEL_COLUMN)         \n",
        "            return features, label\n",
        "\n",
        "        # Create list of files that match pattern\n",
        "        file_list = tf.gfile.Glob(filename = filename)\n",
        "\n",
        "        # Create dataset from file list\n",
        "        dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
        "        res = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "        # if res.keys() in df_cross.columns:\n",
        "        #   return dataset.make_one_shot_iterator().get_next()\n",
        "        # else:\n",
        "        return res\n",
        "    return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2G8EtDxWARg"
      },
      "source": [
        "num_listing_embedding = 100\n",
        "#num_reviewer_embedding = 100\n",
        "\n",
        "# Create feature columns to be used in model\n",
        "def create_feature_columns():\n",
        "    # Create listing id feature column\n",
        "    listing_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "        key = \"listing_id\",\n",
        "        hash_bucket_size = number_of_listing_id)\n",
        "    \n",
        "    embedded_listing_id_column = tf.feature_column.embedding_column(\n",
        "        categorical_column = listing_id_column,\n",
        "        dimension = num_listing_embedding)    \n",
        "\n",
        "    # Create reviewer id feature column\n",
        "    # reviewer_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "    #     key = \"reviewer_id\",\n",
        "    #     hash_bucket_size = number_of_reviewer_id)\n",
        "    \n",
        "    # embedded_reviewer_id_column = tf.feature_column.embedding_column(\n",
        "    #     categorical_column = reviewer_id_column,\n",
        "    #     dimension = num_reviewer_embedding)    \n",
        "\n",
        "    # Create superhost feature column\n",
        "    categorical_superhost_column = tf.feature_column.categorical_column_with_identity(\n",
        "        key = 'host_is_superhost',\n",
        "        num_buckets = 2)\n",
        "    \n",
        "    indicator_superhost_column = tf.feature_column.indicator_column(\n",
        "        categorical_column = categorical_superhost_column\n",
        "    )\n",
        "\n",
        "    # Create bucketized price column\n",
        "    bucketized_price_column = tf.feature_column.bucketized_column(\n",
        "        tf.feature_column.numeric_column(key = 'price'),\n",
        "        boundaries=list(np.arange(0,1000,100)))\n",
        "\n",
        "    # Create numeric columns: 'number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "    #'nearest_station_dist','host_response_time_cross_rate'\n",
        "\n",
        "    number_of_reviews_ltm_column = tf.feature_column.numeric_column(\n",
        "        key = 'number_of_reviews_ltm')#, \n",
        "        #normalizer_fn=lambda x:(x-norl_mean) / norl_std)\n",
        "    \n",
        "    calculated_host_listings_count_column = tf.feature_column.numeric_column(\n",
        "        key = 'calculated_host_listings_count')\n",
        "        #,normalizer_fn=lambda x:(x-chlc_mean) / chlc_std)\n",
        "    \n",
        "    nearby_sites_rating_column = tf.feature_column.numeric_column(\n",
        "        key = 'nearby_sites_rating')\n",
        "        #,normalizer_fn=lambda x:(x-nsr_mean) / nsr_std)\n",
        "    \n",
        "    nearest_station_dist_column = tf.feature_column.numeric_column(\n",
        "        key = 'nearest_station_dist')\n",
        "        #,normalizer_fn=lambda x:(x-nsd_mean) / nsd_std)\n",
        "\n",
        "    host_response_time_cross_rate_column = tf.feature_column.numeric_column(\n",
        "        key = 'host_response_time_cross_rate')\n",
        "  \n",
        "\n",
        "    # Create lat,lon boundaries list for our binning\n",
        "    lat_boundaries = list(np.arange(lat_min, lat_max+0.01, 0.05))\n",
        "    lon_boundaries = list(np.arange(lon_min, lon_max+0.01, 0.05))\n",
        "\n",
        "    lat_column = tf.feature_column.numeric_column(\n",
        "        key = \"latitude\")\n",
        "    lon_column = tf.feature_column.numeric_column(\n",
        "        key = \"longitude\")   \n",
        "    \n",
        "    # Create bucketized lat,lon feature column using our boundaries\n",
        "    lat_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lat_column,\n",
        "        boundaries = lat_boundaries)\n",
        "    \n",
        "    lon_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lon_column,\n",
        "        boundaries = lon_boundaries)    \n",
        "    \n",
        "    # Cross bucketized lat column and bucketized lon column\n",
        "    crossed_lat_lon_column = tf.feature_column.crossed_column(\n",
        "        keys = [lat_bucketized_column, lon_bucketized_column],\n",
        "        hash_bucket_size = len(lat_bucketized_column) * len(lon_bucketized_column))\n",
        "\n",
        "    # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
        "    indicator_crossed_lat_lon_column = tf.feature_column.indicator_column(\n",
        "            categorical_column = crossed_lat_lon_column)    \n",
        "\n",
        "    # Create user and item factor feature columns from our trained WALS model\n",
        "    user_factors = [tf.feature_column.numeric_column(key = \"user_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    item_factors =  [tf.feature_column.numeric_column(key = \"item_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    text_factors = [tf.feature_column.numeric_column(key = \"text_factor_\" + str(i)) for i in range(text_feat)]\n",
        "\n",
        "    # Create list of feature columns\n",
        "    feature_columns = [\n",
        "      embedded_listing_id_column,    \n",
        "      # embedded_reviewer_id_column,       \n",
        "      indicator_superhost_column,\n",
        "      bucketized_price_column,\n",
        "      calculated_host_listings_count_column,\n",
        "      nearby_sites_rating_column,\n",
        "      nearest_station_dist_column,\n",
        "      host_response_time_cross_rate_column,\n",
        "      indicator_crossed_lat_lon_column] + user_factors + item_factors + text_factors\n",
        "\n",
        "    return feature_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJTkDWy3JFob"
      },
      "source": [
        "# Create custom model function for our custom estimator\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Create neural network input layer using our feature columns defined above\n",
        "    net = tf.feature_column.input_layer(features = features, feature_columns = params[\"feature_columns\"])\n",
        "\n",
        "    # Create hidden layers by looping through hidden unit list\n",
        "    for units in params[\"hidden_units\"]:\n",
        "        net = tf.nn.dropout(\n",
        "          net, params['dropout_rate'], seed=1\n",
        "        )\n",
        "        net = tf.layers.dense(inputs = net, \n",
        "                              units = units, \n",
        "                              activation = tf.nn.relu,\n",
        "                              kernel_regularizer = tf.keras.regularizers.l1_l2(l1=params[\"l1\"],l2=params[\"l2\"]))\n",
        "\n",
        "    x_out = tf.layers.Dense(1, activation=tf.nn.sigmoid)(net)\n",
        "\n",
        "    # If the mode is prediction\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Create predictions dict\n",
        "        predictions_dict = {\n",
        "            \"preference\": tf.expand_dims(input = x_out, axis = -1),\n",
        "        }\n",
        "\n",
        "        # Create export outputs\n",
        "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
        "\n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for prediction mode\n",
        "          mode = mode,\n",
        "          predictions = predictions_dict,\n",
        "          loss = None,\n",
        "          train_op = None,\n",
        "          eval_metric_ops = None,\n",
        "          export_outputs = export_outputs)\n",
        "\n",
        "\n",
        "    # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
        "    loss = tf.losses.mean_squared_error(labels, x_out[0])\n",
        "\n",
        "    # If the mode is evaluation\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # Metrics\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, x_out)\n",
        "        mae = tf.metrics.mean_absolute_error(labels, x_out)\n",
        "\n",
        "        # Put eval metrics into a dictionary\n",
        "        eval_metric_ops = {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae}\n",
        "\n",
        "        # Create scalar summaries to see in TensorBoard\n",
        "        tf.summary.scalar(name = \"RMSE\", tensor = rmse[1])\n",
        "        tf.summary.scalar(name = \"MAE\", tensor = mae[1])   \n",
        "        \n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for evaluation mode\n",
        "            mode = mode,\n",
        "            predictions = None,\n",
        "            loss = loss,\n",
        "            train_op = None,\n",
        "            eval_metric_ops = eval_metric_ops,\n",
        "            export_outputs = None)\n",
        "\n",
        "    # Continue on with training mode\n",
        "\n",
        "    # If the mode is training\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "    # Create a custom optimizer\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate = params[\"learning_rate\"])\n",
        "\n",
        "    # Create train op\n",
        "    train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
        "\n",
        "    return tf.estimator.EstimatorSpec( # final return since we\"re done with what we need for training mode\n",
        "        mode = mode,\n",
        "        predictions = None,\n",
        "        loss = loss,\n",
        "        train_op = train_op,\n",
        "        eval_metric_ops = None,\n",
        "        export_outputs = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFienoSnTtkX"
      },
      "source": [
        "# Create serving input function\n",
        "def serving_input_fn():  \n",
        "    feature_placeholders = {\n",
        "        colname : tf.placeholder(dtype = tf.float64, shape = [None]) \\\n",
        "        for colname in NON_FACTOR_COLUMNS[3:]\n",
        "    }\n",
        "\n",
        " #   feature_placeholders[NON_FACTOR_COLUMNS[0]] = tf.placeholder(dtype = tf.string, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[0]] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[1]] = tf.placeholder(dtype = tf.string, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[2]] = tf.placeholder(dtype = tf.int64, shape = [None])\n",
        "\n",
        "    for colname in FACTOR_COLUMNS1 + FACTOR_COLUMNS2:\n",
        "        feature_placeholders[colname] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1) \\\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rQLg9Et24VT"
      },
      "source": [
        "### Train, evaluate and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfzQgbX1TuEQ"
      },
      "source": [
        "# Create train and evaluate loop to combine all of the pieces together.\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "def train_and_evaluate(args):\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn = model_fn,\n",
        "        model_dir = args[\"output_dir\"],\n",
        "        params = {\n",
        "        \"feature_columns\": create_feature_columns(),\n",
        "        \"hidden_units\": args[\"hidden_units\"],\n",
        "        \"learning_rate\": args[\"learning_rate\"],\n",
        "        \"dropout_rate\": args[\"dropout_rate\"],\n",
        "        \"l1\": args[\"l1\"],\n",
        "        \"l2\": args[\"l2\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = read_dataset(filename = args[\"train_data_paths\"], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args[\"batch_size\"]),\n",
        "        max_steps = args[\"train_steps\"])\n",
        "\n",
        "    exporter = tf.estimator.LatestExporter(name = \"exporter\", \n",
        "                                           serving_input_receiver_fn = serving_input_fn)\n",
        "\n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = read_dataset(filename = args[\"eval_data_paths\"], mode = tf.estimator.ModeKeys.EVAL, batch_size = args[\"batch_size\"]),\n",
        "        steps = None,\n",
        "        start_delay_secs = args[\"start_delay_secs\"],\n",
        "        throttle_secs = args[\"throttle_secs\"],\n",
        "        exporters = exporter)\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
        "    return estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h6HObBCWSlL",
        "outputId": "4caa11eb-7496-4857-89cb-f5751d81417e"
      },
      "source": [
        "# Call train and evaluate loop\n",
        "import shutil\n",
        "\n",
        "outdir = \"hybrid_recommendation_trained\"\n",
        "shutil.rmtree(path = outdir, ignore_errors = True) # start fresh each time\n",
        "\n",
        "arguments = {\n",
        "    \"train_data_paths\": \"train.csv\",\n",
        "    \"eval_data_paths\": \"test.csv\",\n",
        "    \"output_dir\": outdir,\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"dropout_rate\": 0.75,\n",
        "    \"l2\": 0.01,\n",
        "    \"l1\": 0.01,\n",
        "    \"hidden_units\": [512, 256, 128, 64, 32],\n",
        "    \"train_steps\": 1000,\n",
        "    \"start_delay_secs\": 30,\n",
        "    \"throttle_secs\": 30\n",
        "}\n",
        "\n",
        "model = train_and_evaluate(arguments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'hybrid_recommendation_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa725df06d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into hybrid_recommendation_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.06109742, step = 1\n",
            "INFO:tensorflow:global_step/sec: 14.8924\n",
            "INFO:tensorflow:loss = 0.05048555, step = 101 (6.720 sec)\n",
            "INFO:tensorflow:global_step/sec: 16.1133\n",
            "INFO:tensorflow:loss = 0.039557923, step = 201 (6.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 16.3886\n",
            "INFO:tensorflow:loss = 0.13187978, step = 301 (6.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 16.0857\n",
            "INFO:tensorflow:loss = 0.041120358, step = 401 (6.215 sec)\n",
            "INFO:tensorflow:global_step/sec: 16.2182\n",
            "INFO:tensorflow:loss = 0.04776553, step = 501 (6.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 15.9555\n",
            "INFO:tensorflow:loss = 0.040278845, step = 601 (6.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 15.8399\n",
            "INFO:tensorflow:loss = 0.051771283, step = 701 (6.313 sec)\n",
            "INFO:tensorflow:global_step/sec: 15.7793\n",
            "INFO:tensorflow:loss = 0.04248161, step = 801 (6.342 sec)\n",
            "INFO:tensorflow:global_step/sec: 15.6923\n",
            "INFO:tensorflow:loss = 0.04887233, step = 901 (6.370 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into hybrid_recommendation_trained/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-04-08T05:53:27Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2021-04-08-05:53:37\n",
            "INFO:tensorflow:Saving dict for global step 1000: MAE = 0.16845156, RMSE = 0.21065637, global_step = 1000, loss = 0.04458747\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: hybrid_recommendation_trained/export/exporter/temp-b'1617861217'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 0.050848164.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3pyp-LYZMnC",
        "outputId": "dd39ca9d-0c80-4d24-8c61-7b27beb11bba"
      },
      "source": [
        "test_pred_raw = [x['preference'] for x in list(model.predict(input_fn=read_dataset('test.csv',mode=tf.estimator.ModeKeys.PREDICT)))]\n",
        "test_pred = [x[0][0] for x in test_pred_raw]\n",
        "testset_mse = sum(np.square(Xtest.preference - test_pred))/len(test_pred)\n",
        "testset_mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03931196245572145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ll9N754sQAJ",
        "outputId": "96dab872-e361-43be-e3fb-5419684519e2"
      },
      "source": [
        "export_dir = '/content/exported_model/1'\n",
        "model.export_saved_model(export_dir,serving_input_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/exported_model/4/temp-b'1617604220'/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/exported_model/4/temp-b'1617604220'/saved_model.pb\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'/content/exported_model/4/1617604220'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVhQeV5zGnPG",
        "outputId": "7d065916-778a-45da-9049-3edbee76e93a"
      },
      "source": [
        "from tensorflow.contrib import predictor\n",
        "\n",
        "predict_fn = predictor.from_saved_model('/content/exported_model/4/1617604220')\n",
        "predictions = predict_fn(Xtest.astype({'listing_id':'string','number_of_reviews_ltm':'float64','calculated_host_listings_count':'float64'}))\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/exported_model/4/1617604220/variables/variables\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/exported_model/4/1617604220/variables/variables\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'preference': array([[[0.88810277]],\n",
            "\n",
            "       [[0.794929  ]],\n",
            "\n",
            "       [[0.8260367 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[0.8244647 ]],\n",
            "\n",
            "       [[0.8540371 ]],\n",
            "\n",
            "       [[0.8230056 ]]], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}