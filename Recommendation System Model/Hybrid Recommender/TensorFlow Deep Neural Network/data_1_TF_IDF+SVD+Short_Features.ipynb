{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data1: TF-IDF + Short Features.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vDmkqhWGANkO",
        "b_pYFLPhoi3r",
        "UnB1RiyS_wOi",
        "8-9LY20iLIok"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prHt5-RmMQJa"
      },
      "source": [
        "# Read in and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr1V0NXmUXbD",
        "outputId": "2b4cef43-f94a-4aad-cd41-5e973abdcd3a"
      },
      "source": [
        "!gdown --id '1XG6tpnEmp74aD0KR6hI80wsX5KkC1Ixc' # reviews\n",
        "!gdown --id '1DCu90-aikc9TP1wTo1h2_CVSRxYLkvyY' # listings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XG6tpnEmp74aD0KR6hI80wsX5KkC1Ixc\n",
            "To: /content/customers_final_version.csv\n",
            "42.8MB [00:00, 91.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DCu90-aikc9TP1wTo1h2_CVSRxYLkvyY\n",
            "To: /content/listings_final_version.csv\n",
            "22.5MB [00:00, 137MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgqgGoCi3B9F"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cmcvGVO3Ccd"
      },
      "source": [
        "listings=pd.read_csv(\"listings_final_version.csv\")\n",
        "customers=pd.read_csv(\"customers_final_version.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCKJqZuQfV_r",
        "outputId": "48abe21b-bd01-4a8f-96f6-0c3d306ce25c"
      },
      "source": [
        "customers.polarity_score.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    130602.000000\n",
              "mean          0.757119\n",
              "std           0.314819\n",
              "min          -0.995800\n",
              "25%           0.680800\n",
              "50%           0.875000\n",
              "75%           0.950400\n",
              "max           0.999700\n",
              "Name: polarity_score, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH-aoGOMgLwg",
        "outputId": "437d803c-b048-409d-934a-44877268c4d6"
      },
      "source": [
        "df_pair[df_pair.host_is_superhost == 0].polarity_score.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7289552548024946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFQnjypMhTmj",
        "outputId": "7200fdf4-486a-4903-a62b-46e7292ee434"
      },
      "source": [
        "df_pair[df_pair.host_is_superhost == 1].polarity_score.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8193620239796193"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzfsjqz7hZJR",
        "outputId": "51afce40-2bb3-49e3-8320-980a2c4b719d"
      },
      "source": [
        "df_pair.host_is_superhost.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    87558\n",
              "1    32194\n",
              "Name: host_is_superhost, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5twMJ0c3PaA",
        "outputId": "77a230c2-a8a2-4b0f-be2a-a37ac3c172d4"
      },
      "source": [
        "listings.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12222 entries, 0 to 12221\n",
            "Data columns (total 68 columns):\n",
            " #   Column                          Non-Null Count  Dtype  \n",
            "---  ------                          --------------  -----  \n",
            " 0   id                              12222 non-null  int64  \n",
            " 1   description                     12222 non-null  object \n",
            " 2   host_since                      12222 non-null  object \n",
            " 3   host_is_superhost               12222 non-null  int64  \n",
            " 4   latitude                        12222 non-null  float64\n",
            " 5   longitude                       12222 non-null  float64\n",
            " 6   accommodates                    12222 non-null  int64  \n",
            " 7   bathrooms                       12222 non-null  float64\n",
            " 8   bedrooms                        12222 non-null  float64\n",
            " 9   beds                            12222 non-null  float64\n",
            " 10  amenities                       12222 non-null  object \n",
            " 11  price                           12222 non-null  float64\n",
            " 12  minimum_nights                  12222 non-null  int64  \n",
            " 13  maximum_nights                  12222 non-null  int64  \n",
            " 14  maximum_nights_avg_ntm          12222 non-null  float64\n",
            " 15  number_of_reviews_ltm           12222 non-null  int64  \n",
            " 16  instant_bookable                12222 non-null  int64  \n",
            " 17  calculated_host_listings_count  12222 non-null  int64  \n",
            " 18  nearby_sites_rating             12222 non-null  float64\n",
            " 19  nearest_station_dist            12222 non-null  float64\n",
            " 20  nearest_site_dist               12222 non-null  float64\n",
            " 21  nearest_site_rating             12222 non-null  float64\n",
            " 22  host_response_time_cross_rate   12222 non-null  float64\n",
            " 23  email                           12222 non-null  int64  \n",
            " 24  phone                           12222 non-null  int64  \n",
            " 25  reviews                         12222 non-null  int64  \n",
            " 26  jumio                           12222 non-null  int64  \n",
            " 27  offline_government_id           12222 non-null  int64  \n",
            " 28  selfie                          12222 non-null  int64  \n",
            " 29  government_id                   12222 non-null  int64  \n",
            " 30  identity_manual                 12222 non-null  int64  \n",
            " 31  work_email                      12222 non-null  int64  \n",
            " 32  manual_online                   12222 non-null  int64  \n",
            " 33  manual_offline                  12222 non-null  int64  \n",
            " 34  x0_Central Region               12222 non-null  float64\n",
            " 35  x0_East Region                  12222 non-null  float64\n",
            " 36  x0_North Region                 12222 non-null  float64\n",
            " 37  x0_North-East Region            12222 non-null  float64\n",
            " 38  x0_West Region                  12222 non-null  float64\n",
            " 39  x0_apartment                    12222 non-null  float64\n",
            " 40  x0_bed and breakfast            12222 non-null  float64\n",
            " 41  x0_boat                         12222 non-null  float64\n",
            " 42  x0_boutique hotel               12222 non-null  float64\n",
            " 43  x0_bungalow                     12222 non-null  float64\n",
            " 44  x0_condominium                  12222 non-null  float64\n",
            " 45  x0_guest suite                  12222 non-null  float64\n",
            " 46  x0_hostel                       12222 non-null  float64\n",
            " 47  x0_hotel                        12222 non-null  float64\n",
            " 48  x0_house                        12222 non-null  float64\n",
            " 49  x0_loft                         12222 non-null  float64\n",
            " 50  x0_other                        12222 non-null  float64\n",
            " 51  x0_townhouse                    12222 non-null  float64\n",
            " 52  x0_Entire home/apt              12222 non-null  float64\n",
            " 53  x0_Hotel room                   12222 non-null  float64\n",
            " 54  x0_Private room                 12222 non-null  float64\n",
            " 55  x0_Shared room                  12222 non-null  float64\n",
            " 56  amenities_list                  12222 non-null  object \n",
            " 57  Wifi                            12222 non-null  int64  \n",
            " 58  Air conditioning                12222 non-null  int64  \n",
            " 59  Kitchen                         12222 non-null  int64  \n",
            " 60  Parking                         12222 non-null  int64  \n",
            " 61  Outdoor                         12222 non-null  int64  \n",
            " 62  Kids                            12222 non-null  int64  \n",
            " 63  Pets allowed                    12222 non-null  int64  \n",
            " 64  Workspace                       12222 non-null  int64  \n",
            " 65  Gym                             12222 non-null  int64  \n",
            " 66  Breakfast                       12222 non-null  int64  \n",
            " 67  availability_average            12222 non-null  float64\n",
            "dtypes: float64(35), int64(29), object(4)\n",
            "memory usage: 6.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6msZXGK3S-q",
        "outputId": "53ee544c-e1df-4f7b-973a-3fd0b31b484e"
      },
      "source": [
        "customers.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 130602 entries, 0 to 130601\n",
            "Data columns (total 14 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   reviewer_id                  130602 non-null  int64  \n",
            " 1   listing_id                   130602 non-null  int64  \n",
            " 2   date                         130602 non-null  object \n",
            " 3   comments                     130602 non-null  object \n",
            " 4   price                        130602 non-null  float64\n",
            " 5   review_scores_rating         130602 non-null  float64\n",
            " 6   review_scores_accuracy       130602 non-null  float64\n",
            " 7   review_scores_cleanliness    130602 non-null  float64\n",
            " 8   review_scores_checkin        130602 non-null  float64\n",
            " 9   review_scores_communication  130602 non-null  float64\n",
            " 10  review_scores_location       130602 non-null  float64\n",
            " 11  review_scores_value          130602 non-null  float64\n",
            " 12  polarity_score               130602 non-null  float64\n",
            " 13  sentiment                    130602 non-null  object \n",
            "dtypes: float64(9), int64(2), object(3)\n",
            "memory usage: 13.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAyULNqvuD3O",
        "outputId": "e2154e3c-986a-4f78-9286-ee0862e32a91"
      },
      "source": [
        "listings.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'description', 'host_since', 'host_is_superhost', 'latitude',\n",
              "       'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds',\n",
              "       'amenities', 'price', 'minimum_nights', 'maximum_nights',\n",
              "       'maximum_nights_avg_ntm', 'number_of_reviews_ltm', 'instant_bookable',\n",
              "       'calculated_host_listings_count', 'nearby_sites_rating',\n",
              "       'nearest_station_dist', 'nearest_site_dist', 'nearest_site_rating',\n",
              "       'host_response_time_cross_rate', 'email', 'phone', 'reviews', 'jumio',\n",
              "       'offline_government_id', 'selfie', 'government_id', 'identity_manual',\n",
              "       'work_email', 'manual_online', 'manual_offline', 'x0_Central Region',\n",
              "       'x0_East Region', 'x0_North Region', 'x0_North-East Region',\n",
              "       'x0_West Region', 'x0_apartment', 'x0_bed and breakfast', 'x0_boat',\n",
              "       'x0_boutique hotel', 'x0_bungalow', 'x0_condominium', 'x0_guest suite',\n",
              "       'x0_hostel', 'x0_hotel', 'x0_house', 'x0_loft', 'x0_other',\n",
              "       'x0_townhouse', 'x0_Entire home/apt', 'x0_Hotel room',\n",
              "       'x0_Private room', 'x0_Shared room', 'amenities_list', 'Wifi',\n",
              "       'Air conditioning', 'Kitchen', 'Parking', 'Outdoor', 'Kids',\n",
              "       'Pets allowed', 'Workspace', 'Gym', 'Breakfast',\n",
              "       'availability_average'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auraA6Zhw6nP"
      },
      "source": [
        "# Combine description and amenties.\n",
        "listings['dtld_descr'] = listings['description'].str.cat(listings['amenities_list'].apply(lambda x:' '.join(x[1:-1].split(',')).replace(\"'\",'')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7abQ3i8tAAA"
      },
      "source": [
        "# Hybrid Recommender\n",
        "\n",
        "1. Baseline linear model\n",
        "2. Random Forest\n",
        "3. XGBoost\n",
        "4. Deep Neural Network\n",
        "\n",
        "*Side note:* <br>\n",
        "Tensorflow version is 1.15. <br>\n",
        "*Lazy* approach. Hard to maintain but efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEx8MHAlqvT"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMHS1jztLMX"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3rP05AfPgSB"
      },
      "source": [
        "# Determine csv and label columns\n",
        "num_factor = 10\n",
        "text_feat = 100\n",
        "\n",
        "NON_FACTOR_COLUMNS = [\"preference\",'listing_id',\n",
        "       'host_is_superhost', 'latitude',\n",
        "       'longitude','price','number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "       'nearest_station_dist', 'host_response_time_cross_rate']\n",
        "FACTOR_COLUMNS1 = [\"user_factor_{}\".format(i) for i in range(num_factor)] + [\"item_factor_{}\".format(i) for i in range(num_factor)]\n",
        "FACTOR_COLUMNS2 = [\"text_factor_{}\".format(i) for i in range(text_feat)] \n",
        "CSV_COLUMNS = NON_FACTOR_COLUMNS + FACTOR_COLUMNS1 + FACTOR_COLUMNS2\n",
        "LABEL_COLUMN = \"preference\"\n",
        "\n",
        "# Set default values for each CSV column\n",
        "NON_FACTOR_DEFAULTS = [[0.0],['Unknown'],[0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
        "FACTOR_DEFAULTS1 = [[0.0] for i in range(num_factor)] + [[0.0] for i in range(num_factor)] # user and item\n",
        "FACTOR_DEFAULTS2 = [[0.0] for i in range(text_feat)]\n",
        "DEFAULTS = NON_FACTOR_DEFAULTS + FACTOR_DEFAULTS1 + FACTOR_DEFAULTS2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyeDCCyrPgSS",
        "outputId": "f947a625-fa69-47bc-cb66-b94bd6007bc3"
      },
      "source": [
        "# Text manipulation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english',max_features=text_feat,ngram_range=(1,1))\n",
        "\n",
        "tfidf_matrix = tfidf.fit_transform(listings['dtld_descr'])\n",
        "\n",
        "tfidf_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12222, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYffGy5EmKbr"
      },
      "source": [
        "df_pair = customers.drop(['price'],axis=1).merge(listings,left_on='listing_id',right_on='id')\n",
        "\n",
        "avg_review = (df_pair.review_scores_rating/100 + (df_pair.review_scores_accuracy + df_pair.review_scores_cleanliness + df_pair.review_scores_checkin + \\\n",
        "df_pair.review_scores_communication + df_pair.review_scores_location + df_pair.review_scores_value)/10)/7\n",
        "\n",
        "df_pair['preference'] = (df_pair['polarity_score']+1)*avg_review/2\n",
        "df_pair = df_pair[['dtld_descr']+NON_FACTOR_COLUMNS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMyrAEZmvoqR",
        "outputId": "40e8e08f-20d9-4309-d807-bec7e089bf8d"
      },
      "source": [
        "df_pair.nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtld_descr                         6572\n",
              "preference                        66281\n",
              "listing_id                         7006\n",
              "host_is_superhost                     2\n",
              "latitude                           4217\n",
              "longitude                          4721\n",
              "price                               477\n",
              "number_of_reviews_ltm                92\n",
              "calculated_host_listings_count      139\n",
              "nearby_sites_rating                6829\n",
              "nearest_station_dist               6829\n",
              "host_response_time_cross_rate       139\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "8VrB-UrOmuGV",
        "outputId": "562ef43d-ecb3-47ba-90c0-c94b8743236a"
      },
      "source": [
        "df_pair.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewer_id</th>\n",
              "      <th>preference</th>\n",
              "      <th>listing_id</th>\n",
              "      <th>host_is_superhost</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>price</th>\n",
              "      <th>number_of_reviews_ltm</th>\n",
              "      <th>calculated_host_listings_count</th>\n",
              "      <th>nearby_sites_rating</th>\n",
              "      <th>nearest_station_dist</th>\n",
              "      <th>host_response_time_cross_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.197520e+05</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>1.197520e+05</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>119752.000000</td>\n",
              "      <td>119752.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>9.615453e+07</td>\n",
              "      <td>0.825014</td>\n",
              "      <td>1.725274e+07</td>\n",
              "      <td>0.268839</td>\n",
              "      <td>1.311528</td>\n",
              "      <td>103.857428</td>\n",
              "      <td>135.892453</td>\n",
              "      <td>19.508927</td>\n",
              "      <td>15.305239</td>\n",
              "      <td>1.994652</td>\n",
              "      <td>0.493154</td>\n",
              "      <td>3.148956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.404637e+07</td>\n",
              "      <td>0.161178</td>\n",
              "      <td>1.044045e+07</td>\n",
              "      <td>0.443358</td>\n",
              "      <td>0.027086</td>\n",
              "      <td>0.039913</td>\n",
              "      <td>129.097117</td>\n",
              "      <td>22.354198</td>\n",
              "      <td>30.487381</td>\n",
              "      <td>1.064402</td>\n",
              "      <td>0.371672</td>\n",
              "      <td>1.138508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.770000e+02</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>4.909100e+04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.243870</td>\n",
              "      <td>103.687460</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.847350</td>\n",
              "      <td>0.003055</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.791730e+07</td>\n",
              "      <td>0.772561</td>\n",
              "      <td>8.313733e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.296450</td>\n",
              "      <td>103.840018</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.174453</td>\n",
              "      <td>0.250401</td>\n",
              "      <td>2.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.917222e+07</td>\n",
              "      <td>0.872043</td>\n",
              "      <td>1.652530e+07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.309360</td>\n",
              "      <td>103.852750</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.589049</td>\n",
              "      <td>0.403779</td>\n",
              "      <td>3.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.471904e+08</td>\n",
              "      <td>0.931327</td>\n",
              "      <td>2.416825e+07</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.315930</td>\n",
              "      <td>103.882920</td>\n",
              "      <td>169.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>2.455412</td>\n",
              "      <td>0.594050</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.850473e+08</td>\n",
              "      <td>0.999850</td>\n",
              "      <td>4.728230e+07</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.453790</td>\n",
              "      <td>103.973970</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>191.000000</td>\n",
              "      <td>352.000000</td>\n",
              "      <td>7.146364</td>\n",
              "      <td>3.350734</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        reviewer_id  ...  host_response_time_cross_rate\n",
              "count  1.197520e+05  ...                  119752.000000\n",
              "mean   9.615453e+07  ...                       3.148956\n",
              "std    8.404637e+07  ...                       1.138508\n",
              "min    2.770000e+02  ...                       0.000000\n",
              "25%    2.791730e+07  ...                       2.790000\n",
              "50%    6.917222e+07  ...                       3.800000\n",
              "75%    1.471904e+08  ...                       4.000000\n",
              "max    3.850473e+08  ...                       4.000000\n",
              "\n",
              "[8 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN1gseOxvuAl"
      },
      "source": [
        "rev_uniq = df_pair.reviewer_id.unique()\n",
        "lst_uniq = df_pair.listing_id.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3unwVmtXm9nI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9594959-d7c8-4f0b-d9cd-a173e60d19ec"
      },
      "source": [
        "!pip install surprise\n",
        "from surprise import SVD, Reader, Dataset\n",
        "from surprise.model_selection import KFold\n",
        "\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "data = Dataset.load_from_df(df_pair[['reviewer_id', 'listing_id', 'preference']], reader)\n",
        "\n",
        "SVD = SVD(n_factors=10)\n",
        "\n",
        "SVD.fit(data.build_full_trainset())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting surprise\n",
            "  Downloading https://files.pythonhosted.org/packages/61/de/e5cba8682201fcf9c3719a6fdda95693468ed061945493dea2dd37c5618b/surprise-0.1-py2.py3-none-any.whl\n",
            "Collecting scikit-surprise\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/37/5d334adaf5ddd65da99fc65f6507e0e4599d092ba048f4302fe8775619e8/scikit-surprise-1.1.1.tar.gz (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1617572 sha256=c750425766c940ada1d62da0b6990bac661e66a1bc1655fb864fbe917cf6c196\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/9c/3d/41b419c9d2aff5b6e2b4c0fc8d25c538202834058f9ed110d0\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7ff1ba5e6710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS9Z1JLLt_Hb",
        "outputId": "0df3d7e0-bc34-4875-8611-59651ee4ba18"
      },
      "source": [
        "item_vec = SVD.qi\n",
        "user_vec = SVD.pu\n",
        "\n",
        "print(item_vec.shape,user_vec.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7006, 10) (106648, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPWldTfPti-d"
      },
      "source": [
        "df_cross = df_pair.copy()\n",
        "\n",
        "item_feats = []\n",
        "user_feats = []\n",
        "\n",
        "for (id, pair) in df_cross.iterrows():\n",
        "  item_feats = item_feats + [item_vec[np.where(lst_uniq == pair.listing_id)[0][0]]]\n",
        "  user_feats = user_feats + [user_vec[np.where(rev_uniq == pair.reviewer_id)[0][0]]]\n",
        "\n",
        "for i in range(10):\n",
        "  df_cross[\"item_factor_{}\".format(i)] = pd.DataFrame(item_feats)[i]\n",
        "  df_cross[\"user_factor_{}\".format(i)] = pd.DataFrame(user_feats)[i]\n",
        "# Time: 6min 12s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0HoaPc2b9rW"
      },
      "source": [
        "# Attach the textual features\n",
        "for i in range(text_feat):\n",
        "  df_cross[\"text_factor_{}\".format(i)] = 0.0\n",
        "\n",
        "tf_df = pd.DataFrame(tfidf_matrix.toarray())\n",
        "temp = []\n",
        "\n",
        "for (ind, row) in df_cross.iterrows():\n",
        "  lst_id = row.listing_id\n",
        "  lst_ord = listings[listings.id == lst_id].index[0]\n",
        "  temp.append(list(tf_df.iloc[lst_ord,:].values))\n",
        "\n",
        "# TIME: 1min 44s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvDcZqUQPXDn"
      },
      "source": [
        "temp_df = pd.DataFrame(temp)\n",
        "\n",
        "for i in range(text_feat):\n",
        "  df_cross[\"text_factor_{}\".format(i)] = temp_df[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ-8O5ATDh6T"
      },
      "source": [
        "number_of_listing_id = df_cross.listing_id.nunique()\n",
        "# number_of_reviewer_id = df_cross.reviewer_id.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwIoQuf08jnb"
      },
      "source": [
        "# KEEP A COPY\n",
        "# df_cross.to_csv('df_cross.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP1UjqKi7Kyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146bd98b-9d12-4d93-db02-1aefc40ff131"
      },
      "source": [
        "# IN CASE: df_cross is missing:\n",
        "!gdown --id '1R80ZMlGRXrcd874ei05Ov9WqlRzzaQD_'\n",
        "df_cross = pd.read_csv('df_cross.csv',error_bad_lines=False).iloc[:,1:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1R80ZMlGRXrcd874ei05Ov9WqlRzzaQD_\n",
            "To: /content/df_cross.csv\n",
            "195MB [00:01, 164MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDBepJqmXlW8",
        "outputId": "81c2d841-ca95-4103-d344-509c0b08b82f"
      },
      "source": [
        "df_cross.groupby(['reviewer_id']).count().sort_values(by=['listing_id'])['listing_id']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "reviewer_id\n",
              "277            1\n",
              "116320402      1\n",
              "116319937      1\n",
              "116315076      1\n",
              "116314455      1\n",
              "            ... \n",
              "56833080      23\n",
              "6406727       25\n",
              "20715591      44\n",
              "106345007     57\n",
              "44250196     109\n",
              "Name: listing_id, Length: 106648, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ENyG2S8HsQl"
      },
      "source": [
        "# Export all possible listings\n",
        "all_listing = listings[['id'] + df_pair.columns[3:].tolist()].join(tf_df)\n",
        "all_listing.columns = ['listing_id']+all_listing.columns.tolist()[1:-text_feat]+FACTOR_COLUMNS2\n",
        "\n",
        "customer_record = df_cross[['listing_id','reviewer_id']].groupby(['reviewer_id']).agg(set).reset_index()\n",
        "\n",
        "for i in FACTOR_COLUMNS1[:10]:\n",
        "  customer_record[i] = df_cross[[i,'reviewer_id']].groupby(['reviewer_id']).mean().values\n",
        "\n",
        "past_listing = df_cross.listing_id.unique()\n",
        "count = 0\n",
        "for (ind, listing) in all_listing.iterrows():\n",
        "  if listing.listing_id in past_listing:\n",
        "    all_listing.loc[ind, FACTOR_COLUMNS1[10:]] = df_cross.loc[ind, FACTOR_COLUMNS1[:10]].values\n",
        "  else:\n",
        "    all_listing.loc[ind, FACTOR_COLUMNS1[10:]] = 0\n",
        "    count += 1\n",
        "# TIME: 43s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-JpaOIdVmGy",
        "outputId": "11f01820-d298-4fe9-f916-2aebc01bf7c2"
      },
      "source": [
        "# Non-reviewed listing count\n",
        "count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94Ln3ek5JrX3"
      },
      "source": [
        "# all_listing.to_csv('all_listing.csv',index=False)\n",
        "# customer_record.to_csv('customer_record.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hO08m-250NT",
        "outputId": "489d4a89-93a0-4595-e8f8-154c79165f03"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest = train_test_split(df_cross, test_size=0.2, random_state=1)\n",
        "Xtrain, Xval = train_test_split(Xtrain, test_size=0.2, random_state=1)\n",
        "print(f\"Shape of train data: {Xtrain.shape}\")\n",
        "print(f\"Shape of validation data: {Xval.shape}\")\n",
        "print(f\"Shape of test data: {Xtest.shape}\")\n",
        "\n",
        "Xtrain.to_csv('train.csv',header=False,index=False)\n",
        "Xval.to_csv('val.csv',header=False,index=False)\n",
        "Xtest.to_csv('test.csv',header=False,index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train data: (76640, 131)\n",
            "Shape of validation data: (19161, 131)\n",
            "Shape of test data: (23951, 131)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV6EFaDnfW2K"
      },
      "source": [
        "# df_cross_txt = df_cross.drop(df_cross.columns[-101:-1],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHcRyrgRfsiZ"
      },
      "source": [
        "# df_cross_txt.to_csv('df_cross_txt.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjuROnkB2t-Q"
      },
      "source": [
        "### Make auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19vsrcrdCwfJ"
      },
      "source": [
        "lat_min = df_cross.latitude.min()\n",
        "lat_max = df_cross.latitude.max()\n",
        "\n",
        "lon_min = df_cross.longitude.min()\n",
        "lon_max = df_cross.longitude.max()\n",
        "\n",
        "norl_mean = df_cross.number_of_reviews_ltm.mean()\n",
        "norl_std = df_cross.number_of_reviews_ltm.std()\n",
        "\n",
        "chlc_mean = df_cross.calculated_host_listings_count.mean()\n",
        "chlc_std = df_cross.calculated_host_listings_count.std()\n",
        "\n",
        "nsr_mean = df_cross.nearby_sites_rating.mean()\n",
        "nsr_std = df_cross.nearby_sites_rating.std()\n",
        "\n",
        "nsd_mean = df_cross.nearest_station_dist.mean()\n",
        "nsd_std = df_cross.nearest_station_dist.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-eeQWi_f-rS"
      },
      "source": [
        "# Create input function for train and eval\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "    def _input_fn(): \n",
        "        def decode_csv(value_column):\n",
        "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
        "            features = dict(zip(CSV_COLUMNS, columns))          \n",
        "            label = features.pop(LABEL_COLUMN)         \n",
        "            return features, label\n",
        "\n",
        "        # Create list of files that match pattern\n",
        "        file_list = tf.gfile.Glob(filename = filename)\n",
        "\n",
        "        # Create dataset from file list\n",
        "        dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
        "        res = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "        # if res.keys() in df_cross.columns:\n",
        "        #   return dataset.make_one_shot_iterator().get_next()\n",
        "        # else:\n",
        "        return res\n",
        "    return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2G8EtDxWARg"
      },
      "source": [
        "num_listing_embedding = 100\n",
        "#num_reviewer_embedding = 100\n",
        "\n",
        "# Create feature columns to be used in model\n",
        "def create_feature_columns():\n",
        "    # Create listing id feature column\n",
        "    listing_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "        key = \"listing_id\",\n",
        "        hash_bucket_size = number_of_listing_id)\n",
        "    \n",
        "    embedded_listing_id_column = tf.feature_column.embedding_column(\n",
        "        categorical_column = listing_id_column,\n",
        "        dimension = num_listing_embedding)    \n",
        "\n",
        "    # Create reviewer id feature column\n",
        "    # reviewer_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "    #     key = \"reviewer_id\",\n",
        "    #     hash_bucket_size = number_of_reviewer_id)\n",
        "    \n",
        "    # embedded_reviewer_id_column = tf.feature_column.embedding_column(\n",
        "    #     categorical_column = reviewer_id_column,\n",
        "    #     dimension = num_reviewer_embedding)    \n",
        "\n",
        "    # Create superhost feature column\n",
        "    categorical_superhost_column = tf.feature_column.categorical_column_with_identity(\n",
        "        key = 'host_is_superhost',\n",
        "        num_buckets = 2)\n",
        "    \n",
        "    indicator_superhost_column = tf.feature_column.indicator_column(\n",
        "        categorical_column = categorical_superhost_column\n",
        "    )\n",
        "\n",
        "    # Create bucketized price column\n",
        "    bucketized_price_column = tf.feature_column.bucketized_column(\n",
        "        tf.feature_column.numeric_column(key = 'price'),\n",
        "        boundaries=list(np.arange(0,1000,100)))\n",
        "\n",
        "    # Create numeric columns: 'number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "    #'nearest_station_dist','host_response_time_cross_rate'\n",
        "\n",
        "    number_of_reviews_ltm_column = tf.feature_column.numeric_column(\n",
        "        key = 'number_of_reviews_ltm')#, \n",
        "        #normalizer_fn=lambda x:(x-norl_mean) / norl_std)\n",
        "    \n",
        "    calculated_host_listings_count_column = tf.feature_column.numeric_column(\n",
        "        key = 'calculated_host_listings_count')\n",
        "        #,normalizer_fn=lambda x:(x-chlc_mean) / chlc_std)\n",
        "    \n",
        "    nearby_sites_rating_column = tf.feature_column.numeric_column(\n",
        "        key = 'nearby_sites_rating')\n",
        "        #,normalizer_fn=lambda x:(x-nsr_mean) / nsr_std)\n",
        "    \n",
        "    nearest_station_dist_column = tf.feature_column.numeric_column(\n",
        "        key = 'nearest_station_dist')\n",
        "        #,normalizer_fn=lambda x:(x-nsd_mean) / nsd_std)\n",
        "\n",
        "    host_response_time_cross_rate_column = tf.feature_column.numeric_column(\n",
        "        key = 'host_response_time_cross_rate')\n",
        "  \n",
        "\n",
        "    # Create lat,lon boundaries list for our binning\n",
        "    lat_boundaries = list(np.arange(lat_min, lat_max+0.01, 0.05))\n",
        "    lon_boundaries = list(np.arange(lon_min, lon_max+0.01, 0.05))\n",
        "\n",
        "    lat_column = tf.feature_column.numeric_column(\n",
        "        key = \"latitude\")\n",
        "    lon_column = tf.feature_column.numeric_column(\n",
        "        key = \"longitude\")   \n",
        "    \n",
        "    # Create bucketized lat,lon feature column using our boundaries\n",
        "    lat_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lat_column,\n",
        "        boundaries = lat_boundaries)\n",
        "    \n",
        "    lon_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lon_column,\n",
        "        boundaries = lon_boundaries)    \n",
        "    \n",
        "    # Cross bucketized lat column and bucketized lon column\n",
        "    crossed_lat_lon_column = tf.feature_column.crossed_column(\n",
        "        keys = [lat_bucketized_column, lon_bucketized_column],\n",
        "        hash_bucket_size = len(lat_bucketized_column) * len(lon_bucketized_column))\n",
        "\n",
        "    # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
        "    indicator_crossed_lat_lon_column = tf.feature_column.indicator_column(\n",
        "            categorical_column = crossed_lat_lon_column)    \n",
        "\n",
        "    # Create user and item factor feature columns from our trained WALS model\n",
        "    user_factors = [tf.feature_column.numeric_column(key = \"user_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    item_factors =  [tf.feature_column.numeric_column(key = \"item_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    text_factors = [tf.feature_column.numeric_column(key = \"text_factor_\" + str(i)) for i in range(text_feat)]\n",
        "\n",
        "    # Create list of feature columns\n",
        "    feature_columns = [\n",
        "      embedded_listing_id_column,    \n",
        "      # embedded_reviewer_id_column,       \n",
        "      indicator_superhost_column,\n",
        "      bucketized_price_column,\n",
        "      calculated_host_listings_count_column,\n",
        "      nearby_sites_rating_column,\n",
        "      nearest_station_dist_column,\n",
        "      host_response_time_cross_rate_column,\n",
        "      indicator_crossed_lat_lon_column] + user_factors + item_factors + text_factors\n",
        "\n",
        "    return feature_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJTkDWy3JFob"
      },
      "source": [
        "# Create custom model function for our custom estimator\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Create neural network input layer using our feature columns defined above\n",
        "    net = tf.feature_column.input_layer(features = features, feature_columns = params[\"feature_columns\"])\n",
        "\n",
        "    # Create hidden layers by looping through hidden unit list\n",
        "    for units in params[\"hidden_units\"]:\n",
        "        net = tf.nn.dropout(\n",
        "          net, params['dropout_rate'], seed=1\n",
        "        )\n",
        "        net = tf.layers.dense(inputs = net, \n",
        "                              units = units, \n",
        "                              activation = tf.nn.relu,\n",
        "                              kernel_regularizer = tf.keras.regularizers.l1_l2(l1=params[\"l1\"],l2=params[\"l2\"]))\n",
        "\n",
        "    x_out = tf.layers.Dense(1, activation=tf.nn.sigmoid)(net)\n",
        "\n",
        "    # If the mode is prediction\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Create predictions dict\n",
        "        predictions_dict = {\n",
        "            \"preference\": tf.expand_dims(input = x_out, axis = -1),\n",
        "        }\n",
        "\n",
        "        # Create export outputs\n",
        "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
        "\n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for prediction mode\n",
        "          mode = mode,\n",
        "          predictions = predictions_dict,\n",
        "          loss = None,\n",
        "          train_op = None,\n",
        "          eval_metric_ops = None,\n",
        "          export_outputs = export_outputs)\n",
        "\n",
        "\n",
        "    # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
        "    loss = tf.losses.mean_squared_error(labels, x_out[0])\n",
        "\n",
        "    # If the mode is evaluation\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # Metrics\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, x_out)\n",
        "        mae = tf.metrics.mean_absolute_error(labels, x_out)\n",
        "\n",
        "        # Put eval metrics into a dictionary\n",
        "        eval_metric_ops = {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae}\n",
        "\n",
        "        # Create scalar summaries to see in TensorBoard\n",
        "        tf.summary.scalar(name = \"RMSE\", tensor = rmse[1])\n",
        "        tf.summary.scalar(name = \"MAE\", tensor = mae[1])   \n",
        "        \n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for evaluation mode\n",
        "            mode = mode,\n",
        "            predictions = None,\n",
        "            loss = loss,\n",
        "            train_op = None,\n",
        "            eval_metric_ops = eval_metric_ops,\n",
        "            export_outputs = None)\n",
        "\n",
        "    # Continue on with training mode\n",
        "\n",
        "    # If the mode is training\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "    # Create a custom optimizer\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate = params[\"learning_rate\"])\n",
        "\n",
        "    # Create train op\n",
        "    train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
        "\n",
        "    return tf.estimator.EstimatorSpec( # final return since we\"re done with what we need for training mode\n",
        "        mode = mode,\n",
        "        predictions = None,\n",
        "        loss = loss,\n",
        "        train_op = train_op,\n",
        "        eval_metric_ops = None,\n",
        "        export_outputs = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFienoSnTtkX"
      },
      "source": [
        "# Create serving input function\n",
        "def serving_input_fn():  \n",
        "    feature_placeholders = {\n",
        "        colname : tf.placeholder(dtype = tf.float64, shape = [None]) \\\n",
        "        for colname in NON_FACTOR_COLUMNS[3:]\n",
        "    }\n",
        "\n",
        " #   feature_placeholders[NON_FACTOR_COLUMNS[0]] = tf.placeholder(dtype = tf.string, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[0]] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[1]] = tf.placeholder(dtype = tf.string, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[2]] = tf.placeholder(dtype = tf.int64, shape = [None])\n",
        "\n",
        "    for colname in FACTOR_COLUMNS1 + FACTOR_COLUMNS2:\n",
        "        feature_placeholders[colname] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1) \\\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rQLg9Et24VT"
      },
      "source": [
        "### Train, evaluate and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfzQgbX1TuEQ"
      },
      "source": [
        "# Create train and evaluate loop to combine all of the pieces together.\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "def train_and_evaluate(args):\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn = model_fn,\n",
        "        model_dir = args[\"output_dir\"],\n",
        "        params = {\n",
        "        \"feature_columns\": create_feature_columns(),\n",
        "        \"hidden_units\": args[\"hidden_units\"],\n",
        "        \"learning_rate\": args[\"learning_rate\"],\n",
        "        \"dropout_rate\": args[\"dropout_rate\"],\n",
        "        \"l1\": args[\"l1\"],\n",
        "        \"l2\": args[\"l2\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = read_dataset(filename = args[\"train_data_paths\"], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args[\"batch_size\"]),\n",
        "        max_steps = args[\"train_steps\"])\n",
        "\n",
        "    exporter = tf.estimator.LatestExporter(name = \"exporter\", \n",
        "                                           serving_input_receiver_fn = serving_input_fn)\n",
        "\n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = read_dataset(filename = args[\"eval_data_paths\"], mode = tf.estimator.ModeKeys.EVAL, batch_size = args[\"batch_size\"]),\n",
        "        steps = None,\n",
        "        start_delay_secs = args[\"start_delay_secs\"],\n",
        "        throttle_secs = args[\"throttle_secs\"],\n",
        "        exporters = exporter)\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
        "    return estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h6HObBCWSlL",
        "outputId": "4caa11eb-7496-4857-89cb-f5751d81417e"
      },
      "source": [
        "# Call train and evaluate loop\n",
        "import shutil\n",
        "\n",
        "outdir = \"hybrid_recommendation_trained\"\n",
        "shutil.rmtree(path = outdir, ignore_errors = True) # start fresh each time\n",
        "\n",
        "arguments = {\n",
        "    \"train_data_paths\": \"train.csv\",\n",
        "    \"eval_data_paths\": \"test.csv\",\n",
        "    \"output_dir\": outdir,\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"dropout_rate\": 0.75,\n",
        "    \"l2\": 0.01,\n",
        "    \"l1\": 0.01,\n",
        "    \"hidden_units\": [512, 256, 128, 64, 32],\n",
        "    \"train_steps\": 1000,\n",
        "    \"start_delay_secs\": 30,\n",
        "    \"throttle_secs\": 30\n",
        "}\n",
        "\n",
        "model = train_and_evaluate(arguments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'hybrid_recommendation_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa725df06d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into hybrid_recommendation_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.06109742, step = 1\n",
            "INFO:tensorflow:global_step/sec: 14.8924\n",
            "INFO:tensorflow:loss = 0.05048555, step = 101 (6.720 sec)\n",
            "INFO:tensorflow:global_step/sec: 16.1133\n",
            "INFO:tensorflow:loss = 0.039557923, step = 201 (6.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 16.3886\n",
            "INFO:tensorflow:loss = 0.13187978, step = 301 (6.100 sec)\n",
            "INFO:tensorflow:global_step/sec: 16.0857\n",
            "INFO:tensorflow:loss = 0.041120358, step = 401 (6.215 sec)\n",
            "INFO:tensorflow:global_step/sec: 16.2182\n",
            "INFO:tensorflow:loss = 0.04776553, step = 501 (6.169 sec)\n",
            "INFO:tensorflow:global_step/sec: 15.9555\n",
            "INFO:tensorflow:loss = 0.040278845, step = 601 (6.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 15.8399\n",
            "INFO:tensorflow:loss = 0.051771283, step = 701 (6.313 sec)\n",
            "INFO:tensorflow:global_step/sec: 15.7793\n",
            "INFO:tensorflow:loss = 0.04248161, step = 801 (6.342 sec)\n",
            "INFO:tensorflow:global_step/sec: 15.6923\n",
            "INFO:tensorflow:loss = 0.04887233, step = 901 (6.370 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into hybrid_recommendation_trained/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-04-08T05:53:27Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2021-04-08-05:53:37\n",
            "INFO:tensorflow:Saving dict for global step 1000: MAE = 0.16845156, RMSE = 0.21065637, global_step = 1000, loss = 0.04458747\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: hybrid_recommendation_trained/export/exporter/temp-b'1617861217'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 0.050848164.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3pyp-LYZMnC",
        "outputId": "dd39ca9d-0c80-4d24-8c61-7b27beb11bba"
      },
      "source": [
        "test_pred_raw = [x['preference'] for x in list(model.predict(input_fn=read_dataset('test.csv',mode=tf.estimator.ModeKeys.PREDICT)))]\n",
        "test_pred = [x[0][0] for x in test_pred_raw]\n",
        "testset_mse = sum(np.square(Xtest.preference - test_pred))/len(test_pred)\n",
        "testset_mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03931196245572145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ll9N754sQAJ",
        "outputId": "96dab872-e361-43be-e3fb-5419684519e2"
      },
      "source": [
        "export_dir = '/content/exported_model/1'\n",
        "model.export_saved_model(export_dir,serving_input_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/exported_model/4/temp-b'1617604220'/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/exported_model/4/temp-b'1617604220'/saved_model.pb\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'/content/exported_model/4/1617604220'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVhQeV5zGnPG",
        "outputId": "7d065916-778a-45da-9049-3edbee76e93a"
      },
      "source": [
        "from tensorflow.contrib import predictor\n",
        "\n",
        "predict_fn = predictor.from_saved_model('/content/exported_model/4/1617604220')\n",
        "predictions = predict_fn(Xtest.astype({'listing_id':'string','number_of_reviews_ltm':'float64','calculated_host_listings_count':'float64'}))\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/exported_model/4/1617604220/variables/variables\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/exported_model/4/1617604220/variables/variables\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'preference': array([[[0.88810277]],\n",
            "\n",
            "       [[0.794929  ]],\n",
            "\n",
            "       [[0.8260367 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[0.8244647 ]],\n",
            "\n",
            "       [[0.8540371 ]],\n",
            "\n",
            "       [[0.8230056 ]]], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}