{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data 3: TF-IDF+Full_listing_feat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7abQ3i8tAAA"
      },
      "source": [
        "# Hybrid Recommender using Deep Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-PuxGveQBcA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6mHTKCEPzCl",
        "outputId": "1c4d802e-b2c8-471a-9815-9d6f8286ad94"
      },
      "source": [
        "!gdown --id '1-30VFKfftZx-H8ex6BmSwKdG5TE0qO6k'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-30VFKfftZx-H8ex6BmSwKdG5TE0qO6k\n",
            "To: /content/Data_3 (TF-IDF + Most_Listings_Features + SVD).csv\n",
            "219MB [00:01, 131MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEgSlAVQQ_k"
      },
      "source": [
        "df_cross = pd.read_csv('/content/Data_3 (TF-IDF + Most_Listings_Features + SVD).csv').iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCE2horkXBuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc4e4e98-a3e1-414c-8b6c-306b50998029"
      },
      "source": [
        "df_cross.columns.to_list()[:30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['reviewer_id',\n",
              " 'preference',\n",
              " 'listing_id',\n",
              " 'host_is_superhost',\n",
              " 'latitude',\n",
              " 'longitude',\n",
              " 'price',\n",
              " 'number_of_reviews_ltm',\n",
              " 'calculated_host_listings_count',\n",
              " 'nearby_sites_rating',\n",
              " 'nearest_station_dist',\n",
              " 'host_response_time_cross_rate',\n",
              " 'item_factor_0',\n",
              " 'user_factor_0',\n",
              " 'item_factor_1',\n",
              " 'user_factor_1',\n",
              " 'item_factor_2',\n",
              " 'user_factor_2',\n",
              " 'item_factor_3',\n",
              " 'user_factor_3',\n",
              " 'item_factor_4',\n",
              " 'user_factor_4',\n",
              " 'item_factor_5',\n",
              " 'user_factor_5',\n",
              " 'item_factor_6',\n",
              " 'user_factor_6',\n",
              " 'item_factor_7',\n",
              " 'user_factor_7',\n",
              " 'item_factor_8',\n",
              " 'user_factor_8']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaznFN5eqRj_"
      },
      "source": [
        "cols = df_cross.columns.to_list()\n",
        "cols_new = map(lambda x:x.replace(' ','_'),cols)\n",
        "df_cross.columns = cols_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mjkak_3qmkl",
        "outputId": "7261c168-7cf7-4101-a1ab-71c9653310cf"
      },
      "source": [
        "df_cross.columns[-40:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['offline_government_id', 'selfie', 'government_id', 'identity_manual',\n",
              "       'work_email', 'manual_online', 'manual_offline', 'x0_Central_Region',\n",
              "       'x0_East_Region', 'x0_North_Region', 'x0_North-East_Region',\n",
              "       'x0_West_Region', 'x0_apartment', 'x0_bed_and_breakfast', 'x0_boat',\n",
              "       'x0_boutique_hotel', 'x0_bungalow', 'x0_condominium', 'x0_guest_suite',\n",
              "       'x0_hostel', 'x0_hotel', 'x0_house', 'x0_loft', 'x0_other',\n",
              "       'x0_townhouse', 'x0_Entire_home/apt', 'x0_Hotel_room',\n",
              "       'x0_Private_room', 'x0_Shared_room', 'Wifi', 'Air_conditioning',\n",
              "       'Kitchen', 'Parking', 'Outdoor', 'Kids', 'Pets_allowed', 'Workspace',\n",
              "       'Gym', 'Breakfast', 'availability_average'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMHS1jztLMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66576b90-4b11-4d64-e4a3-6974fb4d0497"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 46.6MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (54.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=9dcb915b4aadc3aca1a02c5c73d376ebff77206907777a1ec5641847cc003905\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, keras-applications, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3rP05AfPgSB"
      },
      "source": [
        "# Determine csv and label columns\n",
        "num_factor = 10\n",
        "text_feat = 100\n",
        "number_of_listing_id = df_cross.listing_id.nunique()\n",
        "\n",
        "NUMERIC_COLUMNS = ['number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "       'nearest_station_dist', 'host_response_time_cross_rate','accommodates',\n",
        " 'bathrooms',\n",
        " 'bedrooms',\n",
        " 'beds',\n",
        " 'minimum_nights',\n",
        " 'maximum_nights',\n",
        " 'nearest_site_dist',\n",
        " 'nearest_site_rating',\n",
        " 'availability_average']\n",
        "\n",
        "INDICATOR_COLUMNS = ['instant_bookable','email',\n",
        " 'phone',\n",
        " 'reviews',\n",
        " 'jumio',\n",
        " 'offline_government_id',\n",
        " 'selfie',\n",
        " 'government_id',\n",
        " 'identity_manual',\n",
        " 'work_email',\n",
        " 'manual_online',\n",
        " 'manual_offline',\n",
        " 'x0_Central_Region',\n",
        " 'x0_East_Region',\n",
        " 'x0_North_Region',\n",
        " 'x0_North-East_Region',\n",
        " 'x0_West_Region',\n",
        " 'x0_apartment',\n",
        " 'x0_bed_and_breakfast',\n",
        " 'x0_boat',\n",
        " 'x0_boutique_hotel',\n",
        " 'x0_bungalow',\n",
        " 'x0_condominium',\n",
        " 'x0_guest_suite',\n",
        " 'x0_hostel',\n",
        " 'x0_hotel',\n",
        " 'x0_house',\n",
        " 'x0_loft',\n",
        " 'x0_other',\n",
        " 'x0_townhouse',\n",
        " 'x0_Entire_home/apt',\n",
        " 'x0_Hotel_room',\n",
        " 'x0_Private_room',\n",
        " 'x0_Shared_room',\n",
        " 'Wifi',\n",
        " 'Air_conditioning',\n",
        " 'Kitchen',\n",
        " 'Parking',\n",
        " 'Outdoor',\n",
        " 'Kids',\n",
        " 'Pets_allowed',\n",
        " 'Workspace',\n",
        " 'Gym',\n",
        " 'Breakfast']\n",
        "NON_FACTOR_COLUMNS = [\"preference\",'listing_id',\n",
        "       'host_is_superhost', 'latitude',\n",
        "       'longitude','price'] + NUMERIC_COLUMNS + INDICATOR_COLUMNS\n",
        "FACTOR_COLUMNS1 = [\"user_factor_{}\".format(i) for i in range(num_factor)] + [\"item_factor_{}\".format(i) for i in range(num_factor)]\n",
        "FACTOR_COLUMNS2 = [\"text_factor_{}\".format(i) for i in range(text_feat)] \n",
        "CSV_COLUMNS = NON_FACTOR_COLUMNS + FACTOR_COLUMNS1 + FACTOR_COLUMNS2\n",
        "LABEL_COLUMN = \"preference\"\n",
        "\n",
        "# Set default values for each CSV column\n",
        "NON_FACTOR_DEFAULTS = [[0.0],['Unknown'],[0],[0.0],[0.0],[0.0]] + [[0.0] for i in range(len(NUMERIC_COLUMNS))] + [[0] for i in range(len(INDICATOR_COLUMNS))]\n",
        "FACTOR_DEFAULTS1 = [[0.0] for i in range(num_factor)] + [[0.0] for i in range(num_factor)] # user and item\n",
        "FACTOR_DEFAULTS2 = [[0.0] for i in range(text_feat)]\n",
        "DEFAULTS = NON_FACTOR_DEFAULTS + FACTOR_DEFAULTS1 + FACTOR_DEFAULTS2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkhpWpZt2KLJ"
      },
      "source": [
        "df_cross = df_cross[CSV_COLUMNS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZnJW0hD233e"
      },
      "source": [
        "df_cross[INDICATOR_COLUMNS] = df_cross[INDICATOR_COLUMNS].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQNdkrMfd8L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42279927-7727-4301-c7a0-bc652e026056"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest = train_test_split(df_cross, test_size=0.2, random_state=1)\n",
        "Xtrain, Xval = train_test_split(Xtrain, test_size=0.3, random_state=1)\n",
        "print(f\"Shape of train data: {Xtrain.shape}\")\n",
        "print(f\"Shape of validation data: {Xval.shape}\")\n",
        "print(f\"Shape of test data: {Xtest.shape}\")\n",
        "\n",
        "# Xtrain.to_csv('train.csv',header=False,index=False)\n",
        "# Xval.to_csv('val.csv',header=False,index=False)\n",
        "# Xtest.to_csv('test.csv',header=False,index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train data: (67060, 184)\n",
            "Shape of validation data: (28741, 184)\n",
            "Shape of test data: (23951, 184)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-eeQWi_f-rS"
      },
      "source": [
        "# Create input function for train and eval\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "    def _input_fn(): \n",
        "        def decode_csv(value_column):\n",
        "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
        "            features = dict(zip(CSV_COLUMNS, columns))          \n",
        "            label = features.pop(LABEL_COLUMN)         \n",
        "            return features, label\n",
        "\n",
        "        # Create list of files that match pattern\n",
        "        file_list = tf.gfile.Glob(filename = filename)\n",
        "\n",
        "        # Create dataset from file list\n",
        "        dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
        "        res = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "        # if res.keys() in df_cross.columns:\n",
        "        #   return dataset.make_one_shot_iterator().get_next()\n",
        "        # else:\n",
        "        return res\n",
        "    return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2G8EtDxWARg"
      },
      "source": [
        "num_listing_embedding = 100\n",
        "lat_min = df_cross.latitude.min()\n",
        "lat_max = df_cross.latitude.max()\n",
        "lon_min = df_cross.longitude.min()\n",
        "lon_max = df_cross.longitude.max()\n",
        "\n",
        "# Create feature columns to be used in model\n",
        "def create_feature_columns():\n",
        "    # Create listing id feature column\n",
        "    listing_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "        key = \"listing_id\",\n",
        "        hash_bucket_size = number_of_listing_id)\n",
        "    \n",
        "    embedded_listing_id_column = tf.feature_column.embedding_column(\n",
        "        categorical_column = listing_id_column,\n",
        "        dimension = num_listing_embedding)     \n",
        "\n",
        "    # Create indicator feature column\n",
        "    indicator_columns = []\n",
        "\n",
        "    for col in INDICATOR_COLUMNS:\n",
        "      cat_column = tf.feature_column.categorical_column_with_identity(\n",
        "          key = col,\n",
        "          num_buckets = df_cross[col].nunique())\n",
        "       \n",
        "      indicator_column = tf.feature_column.indicator_column(\n",
        "        categorical_column = cat_column)    \n",
        "      \n",
        "      indicator_columns.append(indicator_column)\n",
        "\n",
        "\n",
        "    categorical_superhost_column = tf.feature_column.categorical_column_with_identity(\n",
        "        key = 'host_is_superhost',\n",
        "        num_buckets = 2)\n",
        "    \n",
        "    indicator_superhost_column = tf.feature_column.indicator_column(\n",
        "        categorical_column = categorical_superhost_column\n",
        "    )\n",
        "\n",
        "    # Create bucketized price column\n",
        "    bucketized_price_column = tf.feature_column.bucketized_column(\n",
        "        tf.feature_column.numeric_column(key = 'price'),\n",
        "        boundaries=list(np.arange(0,1000,100)))\n",
        "    \n",
        "    # Create lat,lon boundaries list for our binning\n",
        "    lat_boundaries = list(np.arange(lat_min, lat_max+0.01, 0.05))\n",
        "    lon_boundaries = list(np.arange(lon_min, lon_max+0.01, 0.05))\n",
        "\n",
        "    lat_column = tf.feature_column.numeric_column(\n",
        "        key = \"latitude\")\n",
        "    lon_column = tf.feature_column.numeric_column(\n",
        "        key = \"longitude\")   \n",
        "    \n",
        "    # Create bucketized lat,lon feature column using our boundaries\n",
        "    lat_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lat_column,\n",
        "        boundaries = lat_boundaries)\n",
        "    \n",
        "    lon_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lon_column,\n",
        "        boundaries = lon_boundaries)    \n",
        "    \n",
        "    # Cross bucketized lat column and bucketized lon column\n",
        "    crossed_lat_lon_column = tf.feature_column.crossed_column(\n",
        "        keys = [lat_bucketized_column, lon_bucketized_column],\n",
        "        hash_bucket_size = len(lat_bucketized_column) * len(lon_bucketized_column))\n",
        "\n",
        "    # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
        "    indicator_crossed_lat_lon_column = tf.feature_column.indicator_column(\n",
        "            categorical_column = crossed_lat_lon_column)    \n",
        "    \n",
        "    # Create all numeric columns\n",
        "    numeric_columns = [tf.feature_column.numeric_column(key = col) for col in NUMERIC_COLUMNS]\n",
        "\n",
        "    # Create user and item factor feature columns from our trained WALS model\n",
        "    user_factors = [tf.feature_column.numeric_column(key = \"user_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    item_factors =  [tf.feature_column.numeric_column(key = \"item_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    text_factors = [tf.feature_column.numeric_column(key = \"text_factor_{}\".format(i)) for i in range(text_feat)]\n",
        "\n",
        "    # Create list of feature columns\n",
        "    feature_columns = [\n",
        "      embedded_listing_id_column,          \n",
        "      indicator_superhost_column,\n",
        "      bucketized_price_column,\n",
        "      indicator_crossed_lat_lon_column] + numeric_columns + indicator_columns + user_factors + item_factors + text_factors\n",
        "\n",
        "    return feature_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJTkDWy3JFob"
      },
      "source": [
        "# Create custom model function for our custom estimator\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Create neural network input layer using our feature columns defined above\n",
        "    net = tf.feature_column.input_layer(features = features, feature_columns = params[\"feature_columns\"])\n",
        "\n",
        "    # Create hidden layers by looping through hidden unit list\n",
        "    for units in params[\"hidden_units\"]:\n",
        "        net = tf.layers.dense(inputs = net, units = units, activation = tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.04))\n",
        "        net = tf.nn.dropout(\n",
        "          net, params['dropout_rate'], seed=1\n",
        "        )\n",
        "\n",
        "    x_out = tf.layers.Dense(1, activation=tf.nn.sigmoid)(net)\n",
        "\n",
        "    # If the mode is prediction\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Create predictions dict\n",
        "        predictions_dict = {\n",
        "            \"preference\": tf.expand_dims(input = x_out, axis = -1),\n",
        "        }\n",
        "\n",
        "        # Create export outputs\n",
        "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
        "\n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for prediction mode\n",
        "          mode = mode,\n",
        "          predictions = predictions_dict,\n",
        "          loss = None,\n",
        "          train_op = None,\n",
        "          eval_metric_ops = None,\n",
        "          export_outputs = export_outputs)\n",
        "\n",
        "\n",
        "    # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
        "    loss = tf.losses.mean_squared_error(labels, x_out[0])\n",
        "\n",
        "    # If the mode is evaluation\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # Metrics\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, x_out)\n",
        "        mae = tf.metrics.mean_absolute_error(labels, x_out)\n",
        "\n",
        "        # Put eval metrics into a dictionary\n",
        "        eval_metric_ops = {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae}\n",
        "\n",
        "        # Create scalar summaries to see in TensorBoard\n",
        "        tf.summary.scalar(name = \"RMSE\", tensor = rmse[1])\n",
        "        tf.summary.scalar(name = \"MAE\", tensor = mae[1])   \n",
        "        \n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for evaluation mode\n",
        "            mode = mode,\n",
        "            predictions = None,\n",
        "            loss = loss,\n",
        "            train_op = None,\n",
        "            eval_metric_ops = eval_metric_ops,\n",
        "            export_outputs = None)\n",
        "\n",
        "    # Continue on with training mode\n",
        "\n",
        "    # If the mode is training\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "    # Create a custom optimizer\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate = params[\"learning_rate\"])\n",
        "\n",
        "    # Create train op\n",
        "    train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
        "\n",
        "    return tf.estimator.EstimatorSpec( # final return since we\"re done with what we need for training mode\n",
        "        mode = mode,\n",
        "        predictions = None,\n",
        "        loss = loss,\n",
        "        train_op = train_op,\n",
        "        eval_metric_ops = None,\n",
        "        export_outputs = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFienoSnTtkX"
      },
      "source": [
        "# Create serving input function\n",
        "def serving_input_fn():  \n",
        "    feature_placeholders = {\n",
        "        colname : tf.placeholder(dtype = tf.float64, shape = [None]) \\\n",
        "        for colname in NON_FACTOR_COLUMNS[3:-len(INDICATOR_COLUMNS)]\n",
        "    }\n",
        "\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[0]] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[1]] = tf.placeholder(dtype = tf.string, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[2]] = tf.placeholder(dtype = tf.int64, shape = [None])\n",
        "\n",
        "    for colname in NON_FACTOR_COLUMNS[-len(INDICATOR_COLUMNS):]:\n",
        "        feature_placeholders[colname] = tf.placeholder(dtype = tf.int64, shape = [None])\n",
        "\n",
        "\n",
        "    for colname in FACTOR_COLUMNS1 + FACTOR_COLUMNS2:\n",
        "        feature_placeholders[colname] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1) \\\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rQLg9Et24VT"
      },
      "source": [
        "### Train, evaluate and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfzQgbX1TuEQ"
      },
      "source": [
        "# Create train and evaluate loop to combine all of the pieces together.\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "def train_and_evaluate(args):\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn = model_fn,\n",
        "        model_dir = args[\"output_dir\"],\n",
        "        params = {\n",
        "        \"feature_columns\": create_feature_columns(),\n",
        "        \"hidden_units\": args[\"hidden_units\"],\n",
        "        \"learning_rate\": args[\"learning_rate\"],\n",
        "        \"dropout_rate\": args[\"dropout_rate\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = read_dataset(filename = args[\"train_data_paths\"], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args[\"batch_size\"]),\n",
        "        max_steps = args[\"train_steps\"])\n",
        "\n",
        "    exporter = tf.estimator.LatestExporter(name = \"exporter\", \n",
        "                                           serving_input_receiver_fn = serving_input_fn)\n",
        "\n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = read_dataset(filename = args[\"eval_data_paths\"], mode = tf.estimator.ModeKeys.EVAL, batch_size = args[\"batch_size\"]),\n",
        "        steps = None,\n",
        "        start_delay_secs = args[\"start_delay_secs\"],\n",
        "        throttle_secs = args[\"throttle_secs\"],\n",
        "        exporters = exporter)\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
        "    return estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h6HObBCWSlL"
      },
      "source": [
        "# Call train and evaluate loop\n",
        "import shutil\n",
        "\n",
        "outdir = \"hybrid_recommendation_trained\"\n",
        "shutil.rmtree(path = outdir, ignore_errors = True) # start fresh each time\n",
        "\n",
        "arguments = {\n",
        "    \"train_data_paths\": \"train.csv\",\n",
        "    \"eval_data_paths\": \"val.csv\",\n",
        "    \"output_dir\": outdir,\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"dropout_rate\": 0.75,\n",
        "    \"hidden_units\": [256, 128, 64],\n",
        "    \"train_steps\": 800,\n",
        "    \"start_delay_secs\": 30,\n",
        "    \"throttle_secs\": 30\n",
        "}\n",
        "\n",
        "model = train_and_evaluate(arguments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3pyp-LYZMnC",
        "outputId": "806f1852-3d99-4705-dbd7-7d3178282e94"
      },
      "source": [
        "test_pred_raw = [x['preference'] for x in list(model.predict(input_fn=read_dataset('test.csv',mode=tf.estimator.ModeKeys.PREDICT)))]\n",
        "test_pred = [x[0][0] for x in test_pred_raw]\n",
        "testset_mse = sum(np.square(Xtest.preference - test_pred))/len(test_pred)\n",
        "testset_mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.056417282750059854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ll9N754sQAJ"
      },
      "source": [
        "# export_dir = '/content/exported_model/1'\n",
        "# model.export_saved_model(export_dir,serving_input_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVhQeV5zGnPG"
      },
      "source": [
        "# from tensorflow.contrib import predictor\n",
        "\n",
        "# predict_fn = predictor.from_saved_model('/content/exported_model/4/1617604220')\n",
        "# predictions = predict_fn(Xtest.astype({'listing_id':'string','number_of_reviews_ltm':'float64','calculated_host_listings_count':'float64'}))\n",
        "# print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}