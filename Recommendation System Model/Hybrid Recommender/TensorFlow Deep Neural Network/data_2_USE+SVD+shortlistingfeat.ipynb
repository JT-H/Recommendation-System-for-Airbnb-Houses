{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data 2: CNN+shortlistingfeat.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7abQ3i8tAAA"
      },
      "source": [
        "# Hybrid Recommender using Deep Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-PuxGveQBcA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6mHTKCEPzCl",
        "outputId": "dfeba3df-506d-4ae1-a1b7-08a5bc08b262"
      },
      "source": [
        "!gdown --id '1oV4E7RXgoBOeYwi7XIdcrycXUU0O_8Fm'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oV4E7RXgoBOeYwi7XIdcrycXUU0O_8Fm\n",
            "To: /content/Data_2 (CNN + Little_Listing_Features + SVD).csv\n",
            "107MB [00:00, 140MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEgSlAVQQ_k"
      },
      "source": [
        "df_cross = pd.read_csv('/content/Data_2 (USE + Little_Listing_Features + SVD).csv').iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN0MWpL2QcNr"
      },
      "source": [
        "cols = df_cross.columns.to_list()\n",
        "\n",
        "cols[-30:] = map(lambda x:x.replace(' ','_'),cols[-30:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCE2horkXBuV"
      },
      "source": [
        "df_cross.columns = cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMHS1jztLMX"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQNdkrMfd8L3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest = train_test_split(df_cross.drop(['reviewer_id'],axis=1), test_size=0.2, random_state=1)\n",
        "Xtrain, Xval = train_test_split(Xtrain, test_size=0.3, random_state=1)\n",
        "print(f\"Shape of train data: {Xtrain.shape}\")\n",
        "print(f\"Shape of validation data: {Xval.shape}\")\n",
        "print(f\"Shape of test data: {Xtest.shape}\")\n",
        "\n",
        "Xtrain.to_csv('train.csv',header=False,index=False)\n",
        "Xval.to_csv('val.csv',header=False,index=False)\n",
        "Xtest.to_csv('test.csv',header=False,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3rP05AfPgSB"
      },
      "source": [
        "# Determine csv and label columns\n",
        "num_factor = 10\n",
        "text_feat = 30\n",
        "number_of_listing_id = df_cross.listing_id.nunique()\n",
        "\n",
        "NUMERIC_COLUMNS = ['number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "       'nearest_station_dist', 'host_response_time_cross_rate']\n",
        "NON_FACTOR_COLUMNS = [\"preference\",'listing_id',\n",
        "       'host_is_superhost', 'latitude',\n",
        "       'longitude','price','number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "       'nearest_station_dist', 'host_response_time_cross_rate']\n",
        "FACTOR_COLUMNS1 = [\"user_factor_{}\".format(i) for i in range(num_factor)] + [\"item_factor_{}\".format(i) for i in range(num_factor)]\n",
        "FACTOR_COLUMNS2 = [\"principal_component_{}\".format(i) for i in range(1,text_feat+1)] \n",
        "CSV_COLUMNS = NON_FACTOR_COLUMNS + FACTOR_COLUMNS1 + FACTOR_COLUMNS2\n",
        "LABEL_COLUMN = \"preference\"\n",
        "\n",
        "# Set default values for each CSV column\n",
        "NON_FACTOR_DEFAULTS = [[0.0],['Unknown'],[0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
        "FACTOR_DEFAULTS1 = [[0.0] for i in range(num_factor)] + [[0.0] for i in range(num_factor)] # user and item\n",
        "FACTOR_DEFAULTS2 = [[0.0] for i in range(text_feat)]\n",
        "DEFAULTS = NON_FACTOR_DEFAULTS + FACTOR_DEFAULTS1 + FACTOR_DEFAULTS2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-eeQWi_f-rS"
      },
      "source": [
        "# Create input function for train and eval\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "    def _input_fn(): \n",
        "        def decode_csv(value_column):\n",
        "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
        "            features = dict(zip(CSV_COLUMNS, columns))          \n",
        "            label = features.pop(LABEL_COLUMN)         \n",
        "            return features, label\n",
        "\n",
        "        # Create list of files that match pattern\n",
        "        file_list = tf.gfile.Glob(filename = filename)\n",
        "\n",
        "        # Create dataset from file list\n",
        "        dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
        "        res = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "        # if res.keys() in df_cross.columns:\n",
        "        #   return dataset.make_one_shot_iterator().get_next()\n",
        "        # else:\n",
        "        return res\n",
        "    return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2G8EtDxWARg"
      },
      "source": [
        "num_listing_embedding = 100\n",
        "lat_min = df_cross.latitude.min()\n",
        "lat_max = df_cross.latitude.max()\n",
        "lon_min = df_cross.longitude.min()\n",
        "lon_max = df_cross.longitude.max()\n",
        "\n",
        "# Create feature columns to be used in model\n",
        "def create_feature_columns():\n",
        "    # Create listing id feature column\n",
        "    listing_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "        key = \"listing_id\",\n",
        "        hash_bucket_size = number_of_listing_id)\n",
        "    \n",
        "    embedded_listing_id_column = tf.feature_column.embedding_column(\n",
        "        categorical_column = listing_id_column,\n",
        "        dimension = num_listing_embedding)     \n",
        "\n",
        "    # Create superhost feature column\n",
        "    categorical_superhost_column = tf.feature_column.categorical_column_with_identity(\n",
        "        key = 'host_is_superhost',\n",
        "        num_buckets = 2)\n",
        "    \n",
        "    indicator_superhost_column = tf.feature_column.indicator_column(\n",
        "        categorical_column = categorical_superhost_column\n",
        "    )\n",
        "\n",
        "    # Create bucketized price column\n",
        "    bucketized_price_column = tf.feature_column.bucketized_column(\n",
        "        tf.feature_column.numeric_column(key = 'price'),\n",
        "        boundaries=list(np.arange(0,1000,100)))\n",
        "    \n",
        "    # Create lat,lon boundaries list for our binning\n",
        "    lat_boundaries = list(np.arange(lat_min, lat_max+0.01, 0.05))\n",
        "    lon_boundaries = list(np.arange(lon_min, lon_max+0.01, 0.05))\n",
        "\n",
        "    lat_column = tf.feature_column.numeric_column(\n",
        "        key = \"latitude\")\n",
        "    lon_column = tf.feature_column.numeric_column(\n",
        "        key = \"longitude\")   \n",
        "    \n",
        "    # Create bucketized lat,lon feature column using our boundaries\n",
        "    lat_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lat_column,\n",
        "        boundaries = lat_boundaries)\n",
        "    \n",
        "    lon_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lon_column,\n",
        "        boundaries = lon_boundaries)    \n",
        "    \n",
        "    # Cross bucketized lat column and bucketized lon column\n",
        "    crossed_lat_lon_column = tf.feature_column.crossed_column(\n",
        "        keys = [lat_bucketized_column, lon_bucketized_column],\n",
        "        hash_bucket_size = len(lat_bucketized_column) * len(lon_bucketized_column))\n",
        "\n",
        "    # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
        "    indicator_crossed_lat_lon_column = tf.feature_column.indicator_column(\n",
        "            categorical_column = crossed_lat_lon_column)    \n",
        "    \n",
        "    # Create all numeric columns\n",
        "    numeric_columns = []\n",
        "\n",
        "    for col in NUMERIC_COLUMNS:\n",
        "      col_min = df_cross[col].min()\n",
        "      col_max = df_cross[col].max()\n",
        "      step = col_max - col_min\n",
        "      numeric_columns.append(tf.feature_column.numeric_column(key = col,normalizer_fn=lambda x:(x-col_min)/step))\n",
        "\n",
        "    # Create user and item factor feature columns from our trained WALS model\n",
        "    user_factors = [tf.feature_column.numeric_column(key = \"user_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    item_factors =  [tf.feature_column.numeric_column(key = \"item_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    text_factors = [tf.feature_column.numeric_column(key = \"principal_component_{}\".format(i)) for i in range(1,text_feat+1)]\n",
        "\n",
        "    # Create list of feature columns\n",
        "    feature_columns = [\n",
        "      embedded_listing_id_column,          \n",
        "      indicator_superhost_column,\n",
        "      bucketized_price_column,\n",
        "      indicator_crossed_lat_lon_column] + numeric_columns + user_factors + item_factors + text_factors\n",
        "\n",
        "    return feature_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJTkDWy3JFob"
      },
      "source": [
        "# Create custom model function for our custom estimator\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Create neural network input layer using our feature columns defined above\n",
        "    net = tf.feature_column.input_layer(features = features, feature_columns = params[\"feature_columns\"])\n",
        "\n",
        "    # Create hidden layers by looping through hidden unit list\n",
        "    for units in params[\"hidden_units\"]:\n",
        "        net = tf.layers.dense(inputs = net, units = units, activation = tf.nn.relu)\n",
        "        net = tf.nn.dropout(\n",
        "          net, params['dropout_rate'], seed=1\n",
        "        )\n",
        "\n",
        "    x_out = tf.layers.Dense(1, activation=tf.nn.sigmoid)(net)\n",
        "\n",
        "    # If the mode is prediction\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Create predictions dict\n",
        "        predictions_dict = {\n",
        "            \"preference\": tf.expand_dims(input = x_out, axis = -1),\n",
        "        }\n",
        "\n",
        "        # Create export outputs\n",
        "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
        "\n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for prediction mode\n",
        "          mode = mode,\n",
        "          predictions = predictions_dict,\n",
        "          loss = None,\n",
        "          train_op = None,\n",
        "          eval_metric_ops = None,\n",
        "          export_outputs = export_outputs)\n",
        "\n",
        "\n",
        "    # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
        "    loss = tf.losses.mean_squared_error(labels, x_out[0])\n",
        "\n",
        "    # If the mode is evaluation\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # Metrics\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, x_out)\n",
        "        mae = tf.metrics.mean_absolute_error(labels, x_out)\n",
        "\n",
        "        # Put eval metrics into a dictionary\n",
        "        eval_metric_ops = {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae}\n",
        "\n",
        "        # Create scalar summaries to see in TensorBoard\n",
        "        tf.summary.scalar(name = \"RMSE\", tensor = rmse[1])\n",
        "        tf.summary.scalar(name = \"MAE\", tensor = mae[1])   \n",
        "        \n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for evaluation mode\n",
        "            mode = mode,\n",
        "            predictions = None,\n",
        "            loss = loss,\n",
        "            train_op = None,\n",
        "            eval_metric_ops = eval_metric_ops,\n",
        "            export_outputs = None)\n",
        "\n",
        "    # Continue on with training mode\n",
        "\n",
        "    # If the mode is training\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "    # Create a custom optimizer\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate = params[\"learning_rate\"])\n",
        "\n",
        "    # Create train op\n",
        "    train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
        "\n",
        "    return tf.estimator.EstimatorSpec( # final return since we\"re done with what we need for training mode\n",
        "        mode = mode,\n",
        "        predictions = None,\n",
        "        loss = loss,\n",
        "        train_op = train_op,\n",
        "        eval_metric_ops = None,\n",
        "        export_outputs = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFienoSnTtkX"
      },
      "source": [
        "# Create serving input function\n",
        "def serving_input_fn():  \n",
        "    feature_placeholders = {\n",
        "        colname : tf.placeholder(dtype = tf.float64, shape = [None]) \\\n",
        "        for colname in NON_FACTOR_COLUMNS[3:]\n",
        "    }\n",
        "\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[0]] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[1]] = tf.placeholder(dtype = tf.string, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[2]] = tf.placeholder(dtype = tf.int64, shape = [None])\n",
        "\n",
        "    for colname in FACTOR_COLUMNS1 + FACTOR_COLUMNS2:\n",
        "        feature_placeholders[colname] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1) \\\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rQLg9Et24VT"
      },
      "source": [
        "### Train, evaluate and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfzQgbX1TuEQ"
      },
      "source": [
        "# Create train and evaluate loop to combine all of the pieces together.\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "def train_and_evaluate(args):\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn = model_fn,\n",
        "        model_dir = args[\"output_dir\"],\n",
        "        params = {\n",
        "        \"feature_columns\": create_feature_columns(),\n",
        "        \"hidden_units\": args[\"hidden_units\"],\n",
        "        \"learning_rate\": args[\"learning_rate\"],\n",
        "        \"dropout_rate\": args[\"dropout_rate\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = read_dataset(filename = args[\"train_data_paths\"], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args[\"batch_size\"]),\n",
        "        max_steps = args[\"train_steps\"])\n",
        "\n",
        "    exporter = tf.estimator.LatestExporter(name = \"exporter\", \n",
        "                                           serving_input_receiver_fn = serving_input_fn)\n",
        "\n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = read_dataset(filename = args[\"eval_data_paths\"], mode = tf.estimator.ModeKeys.EVAL, batch_size = args[\"batch_size\"]),\n",
        "        steps = None,\n",
        "        start_delay_secs = args[\"start_delay_secs\"],\n",
        "        throttle_secs = args[\"throttle_secs\"],\n",
        "        exporters = exporter)\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
        "    return estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h6HObBCWSlL",
        "outputId": "b3e880ef-f746-4711-d9d9-c548e8dda203"
      },
      "source": [
        "# Call train and evaluate loop\n",
        "import shutil\n",
        "\n",
        "outdir = \"hybrid_recommendation_trained\"\n",
        "shutil.rmtree(path = outdir, ignore_errors = True) # start fresh each time\n",
        "\n",
        "arguments = {\n",
        "    \"train_data_paths\": \"train.csv\",\n",
        "    \"eval_data_paths\": \"val.csv\",\n",
        "    \"output_dir\": outdir,\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"dropout_rate\": 0.75,\n",
        "    \"hidden_units\": [256, 128, 64],\n",
        "    \"train_steps\": 1500,\n",
        "    \"start_delay_secs\": 30,\n",
        "    \"throttle_secs\": 30\n",
        "}\n",
        "\n",
        "model = train_and_evaluate(arguments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'hybrid_recommendation_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdf7f67d510>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into hybrid_recommendation_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.19731985, step = 1\n",
            "INFO:tensorflow:global_step/sec: 23.619\n",
            "INFO:tensorflow:loss = 0.02421572, step = 101 (4.239 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2245\n",
            "INFO:tensorflow:loss = 0.046776723, step = 201 (3.962 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.3072\n",
            "INFO:tensorflow:loss = 0.031048536, step = 301 (4.113 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3714\n",
            "INFO:tensorflow:loss = 0.029651158, step = 401 (3.941 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4308\n",
            "INFO:tensorflow:loss = 0.037600897, step = 501 (3.937 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4029\n",
            "INFO:tensorflow:loss = 0.029394269, step = 601 (3.937 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4427\n",
            "INFO:tensorflow:loss = 0.030598318, step = 701 (3.929 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2702\n",
            "INFO:tensorflow:loss = 0.02603114, step = 801 (3.957 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.371\n",
            "INFO:tensorflow:loss = 0.026358467, step = 901 (3.942 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.075\n",
            "INFO:tensorflow:loss = 0.023427367, step = 1001 (3.985 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.6467\n",
            "INFO:tensorflow:loss = 0.045500044, step = 1101 (3.902 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.8135\n",
            "INFO:tensorflow:loss = 0.017842844, step = 1201 (3.874 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3408\n",
            "INFO:tensorflow:loss = 0.033224378, step = 1301 (3.943 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.7108\n",
            "INFO:tensorflow:loss = 0.03093527, step = 1401 (4.050 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1500 into hybrid_recommendation_trained/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-04-06T14:01:32Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2021-04-06-14:01:39\n",
            "INFO:tensorflow:Saving dict for global step 1500: MAE = 0.11525842, RMSE = 0.16377342, global_step = 1500, loss = 0.026326137\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1500: hybrid_recommendation_trained/model.ckpt-1500\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1500\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: hybrid_recommendation_trained/export/exporter/temp-b'1617717699'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 0.028549403.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3pyp-LYZMnC",
        "outputId": "d04fdd8a-f431-4b99-db39-0fa2c5b9791c"
      },
      "source": [
        "test_pred_raw = [x['preference'] for x in list(model.predict(input_fn=read_dataset('test.csv',mode=tf.estimator.ModeKeys.PREDICT)))]\n",
        "test_pred = [x[0][0] for x in test_pred_raw]\n",
        "testset_mse = sum(np.square(Xtest.preference - test_pred))/len(test_pred)\n",
        "testset_mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.026851866457538398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ll9N754sQAJ"
      },
      "source": [
        "# export_dir = '/content/exported_model/1'\n",
        "# model.export_saved_model(export_dir,serving_input_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVhQeV5zGnPG"
      },
      "source": [
        "# from tensorflow.contrib import predictor\n",
        "\n",
        "# predict_fn = predictor.from_saved_model('/content/exported_model/4/1617604220')\n",
        "# predictions = predict_fn(Xtest.astype({'listing_id':'string','number_of_reviews_ltm':'float64','calculated_host_listings_count':'float64'}))\n",
        "# print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}