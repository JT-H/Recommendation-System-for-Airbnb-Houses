{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data 0: SVD + shortlistingfeat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7abQ3i8tAAA"
      },
      "source": [
        "# Hybrid Recommender using Deep Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-PuxGveQBcA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6mHTKCEPzCl",
        "outputId": "13f01a10-5dc7-46d0-ace7-70ed810e42bb"
      },
      "source": [
        "!gdown --id '1oV4E7RXgoBOeYwi7XIdcrycXUU0O_8Fm'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oV4E7RXgoBOeYwi7XIdcrycXUU0O_8Fm\n",
            "To: /content/Data_2 (CNN + Little_Listing_Features + SVD).csv\n",
            "107MB [00:00, 140MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEgSlAVQQ_k"
      },
      "source": [
        "df_cross = pd.read_csv('/content/Data_2 (CNN + Little_Listing_Features + SVD).csv').iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN0MWpL2QcNr"
      },
      "source": [
        "cols = df_cross.columns.to_list()\n",
        "\n",
        "cols[-30:] = map(lambda x:x.replace(' ','_'),cols[-30:])\n",
        "\n",
        "df_cross.columns = cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCE2horkXBuV"
      },
      "source": [
        "df_cross.drop(df_cross.columns[-30:],axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMHS1jztLMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4295fee-075b-48b5-c702-aa7e99b6dbca"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (54.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=e9463d76d9be1fdadded4e6d2a3e95ea53943d199e359e1a48411cacb5f562b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, keras-applications, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3rP05AfPgSB"
      },
      "source": [
        "# Determine csv and label columns\n",
        "num_factor = 10\n",
        "text_feat = 30\n",
        "number_of_listing_id = df_cross.listing_id.nunique()\n",
        "\n",
        "NUMERIC_COLUMNS = ['number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "       'nearest_station_dist', 'host_response_time_cross_rate']\n",
        "NON_FACTOR_COLUMNS = [\"preference\",'listing_id',\n",
        "       'host_is_superhost', 'latitude',\n",
        "       'longitude','price','number_of_reviews_ltm','calculated_host_listings_count','nearby_sites_rating',\n",
        "       'nearest_station_dist', 'host_response_time_cross_rate']\n",
        "FACTOR_COLUMNS1 = [\"user_factor_{}\".format(i) for i in range(num_factor)] + [\"item_factor_{}\".format(i) for i in range(num_factor)]\n",
        "#FACTOR_COLUMNS2 = [\"principal_component_{}\".format(i) for i in range(1,text_feat+1)] \n",
        "CSV_COLUMNS = NON_FACTOR_COLUMNS + FACTOR_COLUMNS1# + FACTOR_COLUMNS2\n",
        "LABEL_COLUMN = \"preference\"\n",
        "\n",
        "# Set default values for each CSV column\n",
        "NON_FACTOR_DEFAULTS = [[0.0],['Unknown'],[0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
        "FACTOR_DEFAULTS1 = [[0.0] for i in range(num_factor)] + [[0.0] for i in range(num_factor)] # user and item\n",
        "#FACTOR_DEFAULTS2 = [[0.0] for i in range(text_feat)]\n",
        "DEFAULTS = NON_FACTOR_DEFAULTS + FACTOR_DEFAULTS1# + FACTOR_DEFAULTS2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKWQerJW2zFe"
      },
      "source": [
        "df_cross = df_cross[CSV_COLUMNS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQNdkrMfd8L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b9d3ba-c541-46dc-cb2e-59864076b3da"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest = train_test_split(df_cross, test_size=0.2, random_state=1)\n",
        "Xtrain, Xval = train_test_split(Xtrain, test_size=0.3, random_state=1)\n",
        "print(f\"Shape of train data: {Xtrain.shape}\")\n",
        "print(f\"Shape of validation data: {Xval.shape}\")\n",
        "print(f\"Shape of test data: {Xtest.shape}\")\n",
        "\n",
        "Xtrain.to_csv('train.csv',header=False,index=False)\n",
        "Xval.to_csv('val.csv',header=False,index=False)\n",
        "Xtest.to_csv('test.csv',header=False,index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train data: (67060, 31)\n",
            "Shape of validation data: (28741, 31)\n",
            "Shape of test data: (23951, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-eeQWi_f-rS"
      },
      "source": [
        "# Create input function for train and eval\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "    def _input_fn(): \n",
        "        def decode_csv(value_column):\n",
        "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
        "            features = dict(zip(CSV_COLUMNS, columns))          \n",
        "            label = features.pop(LABEL_COLUMN)         \n",
        "            return features, label\n",
        "\n",
        "        # Create list of files that match pattern\n",
        "        file_list = tf.gfile.Glob(filename = filename)\n",
        "\n",
        "        # Create dataset from file list\n",
        "        dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
        "        res = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "        # if res.keys() in df_cross.columns:\n",
        "        #   return dataset.make_one_shot_iterator().get_next()\n",
        "        # else:\n",
        "        return res\n",
        "    return _input_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2G8EtDxWARg"
      },
      "source": [
        "num_listing_embedding = 100\n",
        "lat_min = df_cross.latitude.min()\n",
        "lat_max = df_cross.latitude.max()\n",
        "lon_min = df_cross.longitude.min()\n",
        "lon_max = df_cross.longitude.max()\n",
        "\n",
        "# Create feature columns to be used in model\n",
        "def create_feature_columns():\n",
        "    # Create listing id feature column\n",
        "    listing_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "        key = \"listing_id\",\n",
        "        hash_bucket_size = number_of_listing_id)\n",
        "    \n",
        "    embedded_listing_id_column = tf.feature_column.embedding_column(\n",
        "        categorical_column = listing_id_column,\n",
        "        dimension = num_listing_embedding)     \n",
        "\n",
        "    # Create superhost feature column\n",
        "    categorical_superhost_column = tf.feature_column.categorical_column_with_identity(\n",
        "        key = 'host_is_superhost',\n",
        "        num_buckets = 2)\n",
        "    \n",
        "    indicator_superhost_column = tf.feature_column.indicator_column(\n",
        "        categorical_column = categorical_superhost_column\n",
        "    )\n",
        "\n",
        "    # Create bucketized price column\n",
        "    bucketized_price_column = tf.feature_column.bucketized_column(\n",
        "        tf.feature_column.numeric_column(key = 'price'),\n",
        "        boundaries=list(np.arange(0,1000,100)))\n",
        "    \n",
        "    # Create lat,lon boundaries list for our binning\n",
        "    lat_boundaries = list(np.arange(lat_min, lat_max+0.01, 0.05))\n",
        "    lon_boundaries = list(np.arange(lon_min, lon_max+0.01, 0.05))\n",
        "\n",
        "    lat_column = tf.feature_column.numeric_column(\n",
        "        key = \"latitude\")\n",
        "    lon_column = tf.feature_column.numeric_column(\n",
        "        key = \"longitude\")   \n",
        "    \n",
        "    # Create bucketized lat,lon feature column using our boundaries\n",
        "    lat_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lat_column,\n",
        "        boundaries = lat_boundaries)\n",
        "    \n",
        "    lon_bucketized_column = tf.feature_column.bucketized_column(\n",
        "        source_column = lon_column,\n",
        "        boundaries = lon_boundaries)    \n",
        "    \n",
        "    # Cross bucketized lat column and bucketized lon column\n",
        "    crossed_lat_lon_column = tf.feature_column.crossed_column(\n",
        "        keys = [lat_bucketized_column, lon_bucketized_column],\n",
        "        hash_bucket_size = len(lat_bucketized_column) * len(lon_bucketized_column))\n",
        "\n",
        "    # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
        "    indicator_crossed_lat_lon_column = tf.feature_column.indicator_column(\n",
        "            categorical_column = crossed_lat_lon_column)    \n",
        "    \n",
        "    # Create all numeric columns\n",
        "    numeric_columns = []\n",
        "\n",
        "    for col in NUMERIC_COLUMNS:\n",
        "      col_min = df_cross[col].min()\n",
        "      col_max = df_cross[col].max()\n",
        "      step = col_max - col_min\n",
        "      numeric_columns.append(tf.feature_column.numeric_column(key = col,normalizer_fn=lambda x:(x-col_min)/step))\n",
        "\n",
        "    # Create user and item factor feature columns from our trained WALS model\n",
        "    user_factors = [tf.feature_column.numeric_column(key = \"user_factor_\" + str(i)) for i in range(num_factor)]\n",
        "    item_factors =  [tf.feature_column.numeric_column(key = \"item_factor_\" + str(i)) for i in range(num_factor)]\n",
        "#    text_factors = [tf.feature_column.numeric_column(key = \"principal_component_{}\".format(i)) for i in range(1,text_feat+1)]\n",
        "\n",
        "    # Create list of feature columns\n",
        "    feature_columns = [\n",
        "      embedded_listing_id_column,          \n",
        "      indicator_superhost_column,\n",
        "      bucketized_price_column,\n",
        "      indicator_crossed_lat_lon_column] + numeric_columns + user_factors + item_factors# + text_factors\n",
        "\n",
        "    return feature_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJTkDWy3JFob"
      },
      "source": [
        "# Create custom model function for our custom estimator\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Create neural network input layer using our feature columns defined above\n",
        "    net = tf.feature_column.input_layer(features = features, feature_columns = params[\"feature_columns\"])\n",
        "\n",
        "    # Create hidden layers by looping through hidden unit list\n",
        "    for units in params[\"hidden_units\"]:\n",
        "        net = tf.layers.dense(inputs = net, units = units, activation = tf.nn.relu)\n",
        "        net = tf.nn.dropout(\n",
        "          net, params['dropout_rate'], seed=1\n",
        "        )\n",
        "\n",
        "    x_out = tf.layers.Dense(1, activation=tf.nn.sigmoid)(net)\n",
        "\n",
        "    # If the mode is prediction\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Create predictions dict\n",
        "        predictions_dict = {\n",
        "            \"preference\": tf.expand_dims(input = x_out, axis = -1),\n",
        "        }\n",
        "\n",
        "        # Create export outputs\n",
        "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
        "\n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for prediction mode\n",
        "          mode = mode,\n",
        "          predictions = predictions_dict,\n",
        "          loss = None,\n",
        "          train_op = None,\n",
        "          eval_metric_ops = None,\n",
        "          export_outputs = export_outputs)\n",
        "\n",
        "\n",
        "    # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
        "    loss = tf.losses.mean_squared_error(labels, x_out[0])\n",
        "\n",
        "    # If the mode is evaluation\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # Metrics\n",
        "        rmse = tf.metrics.root_mean_squared_error(labels, x_out)\n",
        "        mae = tf.metrics.mean_absolute_error(labels, x_out)\n",
        "\n",
        "        # Put eval metrics into a dictionary\n",
        "        eval_metric_ops = {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae}\n",
        "\n",
        "        # Create scalar summaries to see in TensorBoard\n",
        "        tf.summary.scalar(name = \"RMSE\", tensor = rmse[1])\n",
        "        tf.summary.scalar(name = \"MAE\", tensor = mae[1])   \n",
        "        \n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for evaluation mode\n",
        "            mode = mode,\n",
        "            predictions = None,\n",
        "            loss = loss,\n",
        "            train_op = None,\n",
        "            eval_metric_ops = eval_metric_ops,\n",
        "            export_outputs = None)\n",
        "\n",
        "    # Continue on with training mode\n",
        "\n",
        "    # If the mode is training\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "    # Create a custom optimizer\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate = params[\"learning_rate\"])\n",
        "\n",
        "    # Create train op\n",
        "    train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
        "\n",
        "    return tf.estimator.EstimatorSpec( # final return since we\"re done with what we need for training mode\n",
        "        mode = mode,\n",
        "        predictions = None,\n",
        "        loss = loss,\n",
        "        train_op = train_op,\n",
        "        eval_metric_ops = None,\n",
        "        export_outputs = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFienoSnTtkX"
      },
      "source": [
        "# Create serving input function\n",
        "def serving_input_fn():  \n",
        "    feature_placeholders = {\n",
        "        colname : tf.placeholder(dtype = tf.float64, shape = [None]) \\\n",
        "        for colname in NON_FACTOR_COLUMNS[3:]\n",
        "    }\n",
        "\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[0]] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[1]] = tf.placeholder(dtype = tf.string, shape = [None])\n",
        "    feature_placeholders[NON_FACTOR_COLUMNS[2]] = tf.placeholder(dtype = tf.int64, shape = [None])\n",
        "\n",
        "    for colname in FACTOR_COLUMNS1:# + FACTOR_COLUMNS2:\n",
        "        feature_placeholders[colname] = tf.placeholder(dtype = tf.float64, shape = [None])\n",
        "\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1) \\\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rQLg9Et24VT"
      },
      "source": [
        "### Train, evaluate and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfzQgbX1TuEQ"
      },
      "source": [
        "# Create train and evaluate loop to combine all of the pieces together.\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "def train_and_evaluate(args):\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn = model_fn,\n",
        "        model_dir = args[\"output_dir\"],\n",
        "        params = {\n",
        "        \"feature_columns\": create_feature_columns(),\n",
        "        \"hidden_units\": args[\"hidden_units\"],\n",
        "        \"learning_rate\": args[\"learning_rate\"],\n",
        "        \"dropout_rate\": args[\"dropout_rate\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = read_dataset(filename = args[\"train_data_paths\"], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args[\"batch_size\"]),\n",
        "        max_steps = args[\"train_steps\"])\n",
        "\n",
        "    exporter = tf.estimator.LatestExporter(name = \"exporter\", \n",
        "                                           serving_input_receiver_fn = serving_input_fn)\n",
        "\n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = read_dataset(filename = args[\"eval_data_paths\"], mode = tf.estimator.ModeKeys.EVAL, batch_size = args[\"batch_size\"]),\n",
        "        steps = None,\n",
        "        start_delay_secs = args[\"start_delay_secs\"],\n",
        "        throttle_secs = args[\"throttle_secs\"],\n",
        "        exporters = exporter)\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
        "    return estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h6HObBCWSlL",
        "outputId": "ab0180cb-4023-4d0d-eed5-2e671183c6c3"
      },
      "source": [
        "# Call train and evaluate loop\n",
        "import shutil\n",
        "\n",
        "outdir = \"hybrid_recommendation_trained\"\n",
        "shutil.rmtree(path = outdir, ignore_errors = True) # start fresh each time\n",
        "\n",
        "arguments = {\n",
        "    \"train_data_paths\": \"train.csv\",\n",
        "    \"eval_data_paths\": \"val.csv\",\n",
        "    \"output_dir\": outdir,\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"dropout_rate\": 0.75,\n",
        "    \"hidden_units\": [128, 64, 32],\n",
        "    \"train_steps\": 1500,\n",
        "    \"start_delay_secs\": 30,\n",
        "    \"throttle_secs\": 30\n",
        "}\n",
        "\n",
        "model = train_and_evaluate(arguments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'hybrid_recommendation_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f01d1627550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into hybrid_recommendation_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.090023525, step = 1\n",
            "INFO:tensorflow:global_step/sec: 39.8506\n",
            "INFO:tensorflow:loss = 0.030772176, step = 101 (2.512 sec)\n",
            "INFO:tensorflow:global_step/sec: 45.4609\n",
            "INFO:tensorflow:loss = 0.019123651, step = 201 (2.202 sec)\n",
            "INFO:tensorflow:global_step/sec: 44.5203\n",
            "INFO:tensorflow:loss = 0.029898625, step = 301 (2.248 sec)\n",
            "INFO:tensorflow:global_step/sec: 44.6931\n",
            "INFO:tensorflow:loss = 0.013817225, step = 401 (2.238 sec)\n",
            "INFO:tensorflow:global_step/sec: 44.0056\n",
            "INFO:tensorflow:loss = 0.037075818, step = 501 (2.270 sec)\n",
            "INFO:tensorflow:global_step/sec: 43.974\n",
            "INFO:tensorflow:loss = 0.037127394, step = 601 (2.272 sec)\n",
            "INFO:tensorflow:global_step/sec: 45.0908\n",
            "INFO:tensorflow:loss = 0.03330797, step = 701 (2.218 sec)\n",
            "INFO:tensorflow:global_step/sec: 44.8959\n",
            "INFO:tensorflow:loss = 0.03744749, step = 801 (2.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 45.3732\n",
            "INFO:tensorflow:loss = 0.014573857, step = 901 (2.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 44.9762\n",
            "INFO:tensorflow:loss = 0.025433091, step = 1001 (2.226 sec)\n",
            "INFO:tensorflow:global_step/sec: 44.1716\n",
            "INFO:tensorflow:loss = 0.029284969, step = 1101 (2.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 43.9884\n",
            "INFO:tensorflow:loss = 0.02824299, step = 1201 (2.277 sec)\n",
            "INFO:tensorflow:global_step/sec: 43.744\n",
            "INFO:tensorflow:loss = 0.021003146, step = 1301 (2.284 sec)\n",
            "INFO:tensorflow:global_step/sec: 43.5442\n",
            "INFO:tensorflow:loss = 0.019379098, step = 1401 (2.300 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1500 into hybrid_recommendation_trained/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-04-06T15:54:06Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2021-04-06-15:54:11\n",
            "INFO:tensorflow:Saving dict for global step 1500: MAE = 0.11386427, RMSE = 0.16141997, global_step = 1500, loss = 0.026084024\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1500: hybrid_recommendation_trained/model.ckpt-1500\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1500\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: hybrid_recommendation_trained/export/exporter/temp-b'1617724451'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 0.026912265.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3pyp-LYZMnC",
        "outputId": "b4139a59-0c95-48c7-b52f-8a0a7678dd49"
      },
      "source": [
        "test_pred_raw = [x['preference'] for x in list(model.predict(input_fn=read_dataset('test.csv',mode=tf.estimator.ModeKeys.PREDICT)))]\n",
        "test_pred = [x[0][0] for x in test_pred_raw]\n",
        "testset_mse = sum(np.square(Xtest.preference - test_pred))/len(test_pred)\n",
        "testset_mse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.028773179027963613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ll9N754sQAJ"
      },
      "source": [
        "# export_dir = '/content/exported_model/1'\n",
        "# model.export_saved_model(export_dir,serving_input_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVhQeV5zGnPG"
      },
      "source": [
        "# from tensorflow.contrib import predictor\n",
        "\n",
        "# predict_fn = predictor.from_saved_model('/content/exported_model/4/1617604220')\n",
        "# predictions = predict_fn(Xtest.astype({'listing_id':'string','number_of_reviews_ltm':'float64','calculated_host_listings_count':'float64'}))\n",
        "# print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}